{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n4 exp 4 temps\\n\\nself dist\\nself dist shifted\\nmlp vanilla\\ncnn vanilla\\ncnn mlp \\ncnn stupider + mlp\\nfidelity of mlp to t' cross fidelity wrt first cnn - 1 plot\\n\\naccuracy NLL ECE \\n\\ntop1 agreement between teacher and student,\\nKL divergence, invariance metric (crossentropy?) -> show patrick it's better\\n\\nFLOPS!!\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "4 exp 4 temps\n",
    "\n",
    "self dist\n",
    "self dist shifted\n",
    "mlp vanilla\n",
    "cnn vanilla\n",
    "cnn mlp \n",
    "cnn stupider + mlp\n",
    "fidelity of mlp to t' cross fidelity wrt first cnn - 1 plot\n",
    "\n",
    "accuracy NLL ECE \n",
    "\n",
    "top1 agreement between teacher and student,\n",
    "KL divergence, invariance metric (crossentropy?) -> show patrick it's better\n",
    "\n",
    "FLOPS!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models.cnn import SimpleCNN\n",
    "from models.resnet import resnet18_mnist\n",
    "from models.mlp import MLP\n",
    "from distillation_utils import Distiller\n",
    "from invariances_utils import shift_preserving_shape, test_IM, validate\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\cuda\\memory.py:444: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_cached())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1\n",
    "#random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "num_epochs = 10\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "TRAIN = False\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "# Define a custom dataset that combines MNIST and additional data\n",
    "class ShiftAugmentedMNIST(Dataset):\n",
    "    def __init__(self, mnist_dataset, translation_times : int = 5, max_shift : int = 5):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        directions = [\"u\",\"d\",\"l\",\"r\"]\n",
    "        self.translations = []\n",
    "        for i in range(len(self.mnist_dataset)):\n",
    "            img, label = self.mnist_dataset[i]\n",
    "            img = img.squeeze()\n",
    "            for t in range(translation_times):\n",
    "                sh = shift_preserving_shape(img, direction=directions[np.random.randint(0,4)],\n",
    "                                            max_shift=max_shift).unsqueeze(0)\n",
    "                if sh is not None:\n",
    "                    self.translations.append((sh, label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.mnist_dataset):\n",
    "            return self.mnist_dataset[index]\n",
    "        else:\n",
    "            return self.translations[index - len(self.mnist_dataset)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset) + len(self.translations)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_augmented_dataset = ShiftAugmentedMNIST(train_dataset)\n",
    "train_augmented_loader = DataLoader(dataset=train_augmented_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undistilled MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Correct normal: 0.9714\n",
      "Correct shifted: 0.3009\n",
      "\n",
      "accu: 0.97153664\n",
      "\n",
      "nlll: 0.10431392\n",
      "\n",
      "ecel: 0.03115511\n",
      "\n",
      "test_IM: 0.89037299\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accu': 0.9715366363525391,\n",
       " 'nlll': 0.1043139174580574,\n",
       " 'ecel': 0.031155109405517578,\n",
       " 'test_IM': tensor(0.8904, device='cuda:0')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading undistilled MLP\n",
    "if TRAIN:\n",
    "    undistilled_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "        hidden_layers= 4, device='cuda')\n",
    "    criterion_mlp = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_mlp = torch.optim.Adam(undistilled_mlp.parameters(), lr=lr)\n",
    "    undistilled_mlp.training_loop(train_loader=train_loader, optimizer=optimizer_mlp, criterion=criterion_mlp, \n",
    "              num_epochs=5, save_path_folder=\"saved_models_undistilled\")\n",
    "if not TRAIN:\n",
    "    undistilled_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models_undistilled/mlp\")\n",
    "\n",
    "validate(model=undistilled_mlp, weights_file=None, valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undistilled MLP Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Correct normal: 0.9691\n",
      "Correct shifted: 0.9673\n",
      "\n",
      "accu: 0.96924764\n",
      "\n",
      "nlll: 0.11307620\n",
      "\n",
      "ecel: 0.03176201\n",
      "\n",
      "test_IM: 0.06156954\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accu': 0.9692476391792297,\n",
       " 'nlll': 0.11307619512081146,\n",
       " 'ecel': 0.0317620113492012,\n",
       " 'test_IM': tensor(0.0616, device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading undistilled MLP augmented\n",
    "if TRAIN:\n",
    "    undistilled_mlp_augmented = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "        hidden_layers= 4, device='cuda')\n",
    "    criterion_mlp = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_mlp = torch.optim.Adam(undistilled_mlp_augmented.parameters(), lr=lr)\n",
    "    undistilled_mlp_augmented.training_loop(train_loader=train_augmented_loader, optimizer=optimizer_mlp, criterion=criterion_mlp, \n",
    "              num_epochs=5, save_path_folder=\"saved_models_undistilled_augmented\")\n",
    "if not TRAIN:\n",
    "    undistilled_mlp_augmented = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models_undistilled_augmented/mlp\")\n",
    "\n",
    "validate(model=undistilled_mlp_augmented, weights_file=\"saved_models_undistilled_augmented/mlp\", valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Resnet18-aps model for MNIST\n",
      "Correct normal: 0.9895\n",
      "Correct shifted: 0.9871\n",
      "\n",
      "accu: 0.98955017\n",
      "\n",
      "nlll: 0.03326803\n",
      "\n",
      "ecel: 0.01363378\n",
      "\n",
      "test_IM: 0.00982955\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accu': 0.9895501732826233,\n",
       " 'nlll': 0.0332680344581604,\n",
       " 'ecel': 0.01363377831876278,\n",
       " 'test_IM': tensor(0.0098, device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtaining CNN\n",
    "cnn_path = \"saved_cnn/model_1\"\n",
    "cnn = resnet18_mnist().to(device)\n",
    "if TRAIN:\n",
    "    criterion_cnn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "    # model training\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            outputs = cnn(images.to('cuda'))\n",
    "            loss = criterion_cnn(outputs, labels.to('cuda'))\n",
    "\n",
    "            optimizer_cnn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_cnn.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    # Save the trained model\n",
    "    torch.save(cnn.state_dict(), cnn_path)\n",
    "    print(f\"Model saved as {cnn_path}!\")\n",
    "if not TRAIN:\n",
    "    state_dict = torch.load(cnn_path)\n",
    "    cnn.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "validate(model=cnn, weights_file=None, valid_data=test_loader, device=device, is_mlp= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Resnet18-aps model for MNIST\n",
      "Correct normal: 0.9911\n",
      "Correct shifted: 0.9907\n",
      "\n",
      "accu: 0.99114251\n",
      "\n",
      "nlll: 0.02678524\n",
      "\n",
      "ecel: 0.01055782\n",
      "\n",
      "test_IM: 0.00580762\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accu': 0.9911425113677979,\n",
       " 'nlll': 0.026785239577293396,\n",
       " 'ecel': 0.010557817295193672,\n",
       " 'test_IM': tensor(0.0058, device='cuda:0')}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: create second \"stupid\" cnn model (at the moment it is the same)\n",
    "#Obtaining CNN\n",
    "cnn_path = \"saved_cnn/model_2\"\n",
    "#cnn = SimpleCNN(in_channels=in_channels, num_classes=num_classes, num_conv_layers=num_conv_layers, temperature=temperature).to('cuda:0')\n",
    "num_epochs = 5\n",
    "cnn = resnet18_mnist().to(device)\n",
    "if TRAIN:\n",
    "    criterion_cnn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "    # model training\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            outputs = cnn(images.to('cuda'))\n",
    "            loss = criterion_cnn(outputs, labels.to('cuda'))\n",
    "\n",
    "            optimizer_cnn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_cnn.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    # Save the trained model\n",
    "    torch.save(cnn.state_dict(), cnn_path)\n",
    "    print(f\"Model saved as {cnn_path}!\")\n",
    "if not TRAIN:\n",
    "    state_dict = torch.load(cnn_path)\n",
    "    cnn.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "validate(model=cnn, weights_file=None, valid_data=test_loader, device=device, is_mlp= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self distilling MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Not using softmax\n",
      "Correct normal: 0.9714\n",
      "Correct shifted: 0.2996\n",
      "\n",
      "accu: 0.97153664\n",
      "\n",
      "nlll: 0.10431392\n",
      "\n",
      "ecel: 0.03115512\n",
      "\n",
      "test_IM: 0.89163655\n",
      "\n",
      "Loading params\n",
      "Correct normal: 0.9355\n",
      "Correct shifted: 0.2326\n",
      "\n",
      "accu: 0.93550956\n",
      "\n",
      "nlll: 0.37540725\n",
      "\n",
      "ecel: 0.05995478\n",
      "\n",
      "test_IM: 1.00565004\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accu': 0.9355095624923706,\n",
       " 'nlll': 0.37540724873542786,\n",
       " 'ecel': 0.05995477735996246,\n",
       " 'test_IM': tensor(1.0057, device='cuda:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Self distilling MLP (only from unshifted data)\n",
    "save_path_folder = \"saved_models_selfdistill_unshifted/\"\n",
    "teacher_path = \"saved_models_undistilled/mlp\"\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_teacher = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device, from_saved_state_dict=teacher_path)\n",
    "\n",
    "validate(model=mlp_teacher, weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "if TRAIN:\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001, is_teacher_mlp=True)\n",
    "    selfdistiller.distill(train_data=train_loader, valid_data=test_loader, save_path_folder= save_path_folder) # TODO the model will not be saved in distill() or is it inherited?? put code for saving student\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder + 'distiller', is_teacher_mlp=True)\n",
    "selfdistiller.compute_fidelity(test_loader)\n",
    "validate(model=selfdistiller.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self distilling MLP Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Not using softmax\n",
      "Correct normal: 0.9691\n",
      "Correct shifted: 0.9661\n",
      "\n",
      "accu: 0.96924764\n",
      "\n",
      "nlll: 0.11307620\n",
      "\n",
      "ecel: 0.03176200\n",
      "\n",
      "test_IM: 0.06044262\n",
      "\n",
      "Loading params\n",
      "Correct normal: 0.9653\n",
      "Correct shifted: 0.3377\n",
      "\n",
      "accu: 0.96546578\n",
      "\n",
      "nlll: 0.15674105\n",
      "\n",
      "ecel: 0.03404006\n",
      "\n",
      "test_IM: 0.87618351\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accu': 0.965465784072876,\n",
       " 'nlll': 0.15674105286598206,\n",
       " 'ecel': 0.03404005616903305,\n",
       " 'test_IM': tensor(0.8762, device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Self distilling MLP (augmented)\n",
    "save_path_folder = \"saved_models_selfdistill_augmented\"\n",
    "teacher_path = \"saved_models_undistilled_augmented/mlp\"\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_teacher = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device, from_saved_state_dict=teacher_path)\n",
    "validate(model=mlp_teacher, weights_file=teacher_path, valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "if TRAIN:\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001, is_teacher_mlp=True)\n",
    "    selfdistiller.distill(train_data=train_loader, valid_data=test_loader, save_path_folder= save_path_folder) # TODO the model is not saved in distill() or is it inherited??\n",
    "    # selfdistiller.test_step(test_loader=test_loader) TODO: there is not test_step method?\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder + 'distiller', is_teacher_mlp=True)\n",
    "    # selfdistiller.test_step(test_loader=test_loader) TODO: there is not test_step method?\n",
    "selfdistiller.compute_fidelity(test_loader)\n",
    "validate(model=mlp_student, weights_file=None, valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distilling CNN_1 -> MLP_1 \n",
    "\n",
    "Distilling CNN_2 -> MLP_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Not using softmax\n",
      "Loading Resnet18-aps model for MNIST\n",
      "Correct normal: 0.9895\n",
      "Correct shifted: 0.986\n",
      "\n",
      "accu: 0.98955017\n",
      "\n",
      "nlll: 0.03326803\n",
      "\n",
      "ecel: 0.01363377\n",
      "\n",
      "test_IM: 0.01049326\n",
      "\n",
      "Loading Resnet18-aps model for MNIST\n",
      "Correct normal: 0.9911\n",
      "Correct shifted: 0.9896\n",
      "\n",
      "accu: 0.99114251\n",
      "\n",
      "nlll: 0.02678524\n",
      "\n",
      "ecel: 0.01055783\n",
      "\n",
      "test_IM: 0.00580127\n",
      "\n",
      "\n",
      "Distilling...\n",
      "Epoch : 1/10\n",
      "Student train loss = 0.80264387\n",
      "Distill train loss = 4.04525003\n",
      "Student train accu = 0.84331690\n",
      "\n",
      "Student valid loss = 0.37215309\n",
      "Distill valid loss = 1.74706853\n",
      "Student valid accu = 0.93242436\n",
      "---\n",
      "Epoch : 2/10\n",
      "Student train loss = 0.32149671\n",
      "Distill train loss = 1.37546878\n",
      "Student train accu = 0.94879398\n",
      "\n",
      "Student valid loss = 0.23742334\n",
      "Distill valid loss = 1.10630628\n",
      "Student valid accu = 0.95650876\n",
      "---\n",
      "Epoch : 3/10\n",
      "Student train loss = 0.23480324\n",
      "Distill train loss = 1.06259051\n",
      "Student train accu = 0.95922175\n",
      "\n",
      "Student valid loss = 0.20575929\n",
      "Distill valid loss = 0.85405749\n",
      "Student valid accu = 0.96357484\n",
      "---\n",
      "Epoch : 4/10\n",
      "Student train loss = 0.18473959\n",
      "Distill train loss = 0.84932451\n",
      "Student train accu = 0.96745069\n",
      "\n",
      "Student valid loss = 0.17263051\n",
      "Distill valid loss = 0.82627044\n",
      "Student valid accu = 0.97024283\n",
      "---\n",
      "Epoch : 5/10\n",
      "Student train loss = 0.15849477\n",
      "Distill train loss = 0.72499094\n",
      "Student train accu = 0.97179837\n",
      "\n",
      "Student valid loss = 0.15313415\n",
      "Distill valid loss = 0.75298578\n",
      "Student valid accu = 0.97223328\n",
      "---\n",
      "Epoch : 6/10\n",
      "Student train loss = 0.14137435\n",
      "Distill train loss = 0.66816041\n",
      "Student train accu = 0.97461354\n",
      "\n",
      "Student valid loss = 0.34324511\n",
      "Distill valid loss = 1.25301126\n",
      "Student valid accu = 0.95043790\n",
      "---\n",
      "Epoch : 7/10\n",
      "Student train loss = 0.13184421\n",
      "Distill train loss = 0.62014331\n",
      "Student train accu = 0.97552972\n",
      "\n",
      "Student valid loss = 0.12623505\n",
      "Distill valid loss = 0.62789867\n",
      "Student valid accu = 0.97651274\n",
      "---\n",
      "Epoch : 8/10\n",
      "Student train loss = 0.11184949\n",
      "Distill train loss = 0.56129122\n",
      "Student train accu = 0.97887793\n",
      "\n",
      "Student valid loss = 0.16035679\n",
      "Distill valid loss = 0.68556166\n",
      "Student valid accu = 0.97253185\n",
      "---\n",
      "Epoch : 9/10\n",
      "Student train loss = 0.11007310\n",
      "Distill train loss = 0.55010589\n",
      "Student train accu = 0.97951093\n",
      "\n",
      "Student valid loss = 0.15578090\n",
      "Distill valid loss = 0.63401703\n",
      "Student valid accu = 0.97183519\n",
      "---\n",
      "Epoch : 10/10\n",
      "Student train loss = 0.09701105\n",
      "Distill train loss = 0.49340720\n",
      "Student train accu = 0.98111007\n",
      "\n",
      "Student valid loss = 0.14249564\n",
      "Distill valid loss = 0.59906896\n",
      "Student valid accu = 0.97263137\n",
      "---\n",
      "Student model saved as saved_models_distill_cnn1_to_mlp1distiller!\n",
      "\n",
      "Distilling...\n",
      "Epoch : 1/10\n",
      "Student train loss = 0.66561871\n",
      "Distill train loss = 3.44625918\n",
      "Student train accu = 0.84844749\n",
      "\n",
      "Student valid loss = 0.30042858\n",
      "Distill valid loss = 1.22839905\n",
      "Student valid accu = 0.94595939\n",
      "---\n",
      "Epoch : 2/10\n",
      "Student train loss = 0.26549572\n",
      "Distill train loss = 1.12900122\n",
      "Student train accu = 0.95200893\n",
      "\n",
      "Student valid loss = 0.24311469\n",
      "Distill valid loss = 0.94888128\n",
      "Student valid accu = 0.95561306\n",
      "---\n",
      "Epoch : 3/10\n",
      "Student train loss = 0.18802654\n",
      "Distill train loss = 0.80638722\n",
      "Student train accu = 0.96395256\n",
      "\n",
      "Student valid loss = 0.17819648\n",
      "Distill valid loss = 0.73383111\n",
      "Student valid accu = 0.96715764\n",
      "---\n",
      "Epoch : 4/10\n",
      "Student train loss = 0.16077893\n",
      "Distill train loss = 0.69855606\n",
      "Student train accu = 0.96886660\n",
      "\n",
      "Student valid loss = 0.16269855\n",
      "Distill valid loss = 0.63928152\n",
      "Student valid accu = 0.96865048\n",
      "---\n",
      "Epoch : 5/10\n",
      "Student train loss = 0.14485384\n",
      "Distill train loss = 0.63098713\n",
      "Student train accu = 0.97139859\n",
      "\n",
      "Student valid loss = 0.14818629\n",
      "Distill valid loss = 0.72010325\n",
      "Student valid accu = 0.97074045\n",
      "---\n",
      "Epoch : 6/10\n",
      "Student train loss = 0.11678876\n",
      "Distill train loss = 0.53110748\n",
      "Student train accu = 0.97664579\n",
      "\n",
      "Student valid loss = 0.13106976\n",
      "Distill valid loss = 0.59078381\n",
      "Student valid accu = 0.97392516\n",
      "---\n",
      "Epoch : 7/10\n",
      "Student train loss = 0.10708952\n",
      "Distill train loss = 0.49085004\n",
      "Student train accu = 0.97772854\n",
      "\n",
      "Student valid loss = 0.13251740\n",
      "Distill valid loss = 0.59119331\n",
      "Student valid accu = 0.97342755\n",
      "---\n",
      "Epoch : 8/10\n",
      "Student train loss = 0.09859899\n",
      "Distill train loss = 0.43845538\n",
      "Student train accu = 0.97992737\n",
      "\n",
      "Student valid loss = 0.12656999\n",
      "Distill valid loss = 0.56336040\n",
      "Student valid accu = 0.97591561\n",
      "---\n",
      "Epoch : 9/10\n",
      "Student train loss = 0.08614195\n",
      "Distill train loss = 0.39813395\n",
      "Student train accu = 0.98222615\n",
      "\n",
      "Student valid loss = 0.15753175\n",
      "Distill valid loss = 0.62912422\n",
      "Student valid accu = 0.96984475\n",
      "---\n",
      "Epoch : 10/10\n",
      "Student train loss = 0.08656173\n",
      "Distill train loss = 0.40365026\n",
      "Student train accu = 0.98154318\n",
      "\n",
      "Student valid loss = 0.10672867\n",
      "Distill valid loss = 0.42640823\n",
      "Student valid accu = 0.97850318\n",
      "---\n",
      "Student model saved as saved_models_distill_cnn2_to_mlp2distiller!\n",
      "Correct normal: 0.9725\n",
      "Correct shifted: 0.3355\n",
      "\n",
      "accu: 0.97263139\n",
      "\n",
      "nlll: 0.14249569\n",
      "\n",
      "ecel: 0.02575893\n",
      "\n",
      "test_IM: 0.89621401\n",
      "\n",
      "Correct normal: 0.9784\n",
      "Correct shifted: 0.3627\n",
      "\n",
      "accu: 0.97850323\n",
      "\n",
      "nlll: 0.10672864\n",
      "\n",
      "ecel: 0.02120134\n",
      "\n",
      "test_IM: 0.86981630\n",
      "\n",
      "Correct normal: 0.9784\n",
      "Correct shifted: 0.3591\n",
      "\n",
      "accu: 0.97850323\n",
      "\n",
      "nlll: 0.10672864\n",
      "\n",
      "ecel: 0.02120134\n",
      "\n",
      "test_IM: 0.87112612\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'top1_agreement': 0.976015127388535, 'teach_stu_kldiv': 0.138189376044535}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Distilling CNN to MLP\n",
    "TRAIN = True\n",
    "save_path_folder_1 = \"saved_models_distill_cnn1_to_mlp1\"\n",
    "save_path_folder_2 = \"saved_models_distill_cnn2_to_mlp2\"\n",
    "teacher_path_1 = \"saved_cnn/model_1\"\n",
    "teacher_path_2 = \"saved_cnn/model_2\"\n",
    "\n",
    "mlp_student_1 = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_student_2 = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "teacher_1 = resnet18_mnist().to(device)\n",
    "state_dict = torch.load(teacher_path_1)\n",
    "teacher_1.load_state_dict(state_dict=state_dict)\n",
    "validate(model=teacher_1, weights_file=None, valid_data=test_loader, device=device, is_mlp= False)\n",
    "\n",
    "#TODO: use different architecture than mlp_teacher_1 (at the moment it is the same)\n",
    "teacher_2 = resnet18_mnist().to(device)\n",
    "state_dict = torch.load(teacher_path_2)\n",
    "teacher_2.load_state_dict(state_dict=state_dict)\n",
    "validate(model=teacher_2, weights_file=None, valid_data=test_loader, device=device, is_mlp= False)\n",
    "\n",
    "if TRAIN:\n",
    "    student1_teacher1 = Distiller(student=mlp_student_1, teacher=teacher_1, device=device, lr=0.001)\n",
    "    student1_teacher1.distill(train_data=train_loader, valid_data=test_loader, save_path_folder= save_path_folder_1) # TODO the model is not saved in distill() or is it inherited??\n",
    "    student1_teacher1.compute_fidelity(test_loader)\n",
    "\n",
    "    student2_teacher2 = Distiller(student=mlp_student_2, teacher=teacher_2, device=device, lr=0.001)\n",
    "    student2_teacher2.distill(train_data=train_loader, valid_data=test_loader, save_path_folder= save_path_folder_2) # TODO the model is not saved in distill() or is it inherited??\n",
    "    student2_teacher2.compute_fidelity(test_loader)\n",
    "\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    student1_teacher1 = Distiller(student=mlp_student_1, teacher=teacher_1, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_1 + 'distiller')\n",
    "    student1_teacher1.compute_fidelity(test_loader)\n",
    "\n",
    "    student2_teacher2 = Distiller(student=mlp_student_2, teacher=teacher_2, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_2 + 'distiller')\n",
    "    student2_teacher2.compute_fidelity(test_loader)\n",
    "\n",
    "validate(model=student1_teacher1.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "validate(model=student2_teacher2.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "student2_teacher1 = Distiller(student=student2_teacher2.get_student(), teacher=teacher_1, device=device, lr=0.001)\n",
    "validate(model=student2_teacher1.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "student2_teacher1.compute_fidelity(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate(model=student1_teacher1.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "Correct normal: 0.9725\n",
    "Correct shifted: 0.3355\n",
    "\n",
    "accu: 0.97263139\n",
    "\n",
    "nlll: 0.14249569\n",
    "\n",
    "ecel: 0.02575893\n",
    "\n",
    "test_IM: 0.89621401\n",
    "\n",
    "-----------------------------------------\n",
    "validate(model=student2_teacher2.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "Correct normal: 0.9784\n",
    "Correct shifted: 0.3627\n",
    "\n",
    "accu: 0.97850323\n",
    "\n",
    "nlll: 0.10672864\n",
    "\n",
    "ecel: 0.02120134\n",
    "\n",
    "test_IM: 0.86981630\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "student1_teacher1.compute_fidelity(test_loader)\n",
    "{'top1_agreement': 0.9727308917197452, 'teach_stu_kldiv': 0.1490013027836841}\n",
    "\n",
    "student2_teacher2.compute_fidelity(test_loader)\n",
    "{'top1_agreement': 0.9791003184713376, 'teach_stu_kldiv': 0.09948632423916574}\n",
    "\n",
    "student2_teacher1.compute_fidelity(test_loader)\n",
    "{'top1_agreement': 0.976015127388535, 'teach_stu_kldiv': 0.138189376044535}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Not using softmax\n",
      "Loading Resnet18-aps model for MNIST\n",
      "Correct normal: 0.9895\n",
      "Correct shifted: 0.9865\n",
      "\n",
      "accu: 0.98955017\n",
      "\n",
      "nlll: 0.03326803\n",
      "\n",
      "ecel: 0.01363378\n",
      "\n",
      "test_IM: 0.00957882\n",
      "\n",
      "Loading Resnet18-aps model for MNIST\n",
      "Correct normal: 0.9911\n",
      "Correct shifted: 0.9907\n",
      "\n",
      "accu: 0.99114251\n",
      "\n",
      "nlll: 0.02678524\n",
      "\n",
      "ecel: 0.01055783\n",
      "\n",
      "test_IM: 0.00578883\n",
      "\n",
      "Loading params\n",
      "Correct normal: 0.9725\n",
      "Correct shifted: 0.3381\n",
      "\n",
      "accu: 0.97263139\n",
      "\n",
      "nlll: 0.14249569\n",
      "\n",
      "ecel: 0.02575892\n",
      "\n",
      "test_IM: 0.89266229\n",
      "\n",
      "Correct normal: 0.9784\n",
      "Correct shifted: 0.3579\n",
      "\n",
      "accu: 0.97850323\n",
      "\n",
      "nlll: 0.10672864\n",
      "\n",
      "ecel: 0.02120135\n",
      "\n",
      "test_IM: 0.87484848\n",
      "\n",
      "Correct normal: 0.9784\n",
      "Correct shifted: 0.3606\n",
      "\n",
      "accu: 0.97850323\n",
      "\n",
      "nlll: 0.10672864\n",
      "\n",
      "ecel: 0.02120134\n",
      "\n",
      "test_IM: 0.86885935\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'top1_agreement': 0.976015127388535, 'teach_stu_kldiv': 0.138189376044535}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Distilling CNN to MLP\n",
    "TRAIN = False\n",
    "save_path_folder_1 = \"saved_models_distill_cnn1_to_mlp1\"\n",
    "save_path_folder_2 = \"saved_models_distill_cnn2_to_mlp2\"\n",
    "teacher_path_1 = \"saved_cnn/model_1\"\n",
    "teacher_path_2 = \"saved_cnn/model_2\"\n",
    "\n",
    "mlp_student_1 = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_student_2 = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "teacher_1 = resnet18_mnist().to(device)\n",
    "state_dict = torch.load(teacher_path_1)\n",
    "teacher_1.load_state_dict(state_dict=state_dict)\n",
    "validate(model=teacher_1, weights_file=None, valid_data=test_loader, device=device, is_mlp= False)\n",
    "\n",
    "#TODO: use different architecture than mlp_teacher_1 (at the moment it is the same)\n",
    "teacher_2 = resnet18_mnist().to(device)\n",
    "state_dict = torch.load(teacher_path_2)\n",
    "teacher_2.load_state_dict(state_dict=state_dict)\n",
    "validate(model=teacher_2, weights_file=None, valid_data=test_loader, device=device, is_mlp= False)\n",
    "\n",
    "if TRAIN:\n",
    "    student1_teacher1 = Distiller(student=mlp_student_1, teacher=teacher_1, device=device, lr=0.001)\n",
    "    student1_teacher1.distill(train_data=train_loader, valid_data=test_loader, save_path_folder= save_path_folder_1) # TODO the model is not saved in distill() or is it inherited??\n",
    "    student1_teacher1.compute_fidelity(test_loader)\n",
    "\n",
    "    student2_teacher2 = Distiller(student=mlp_student_2, teacher=teacher_2, device=device, lr=0.001)\n",
    "    student2_teacher2.distill(train_data=train_loader, valid_data=test_loader, save_path_folder= save_path_folder_2) # TODO the model is not saved in distill() or is it inherited??\n",
    "    student2_teacher2.compute_fidelity(test_loader)\n",
    "\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    student1_teacher1 = Distiller(student=mlp_student_1, teacher=teacher_1, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_1 + 'distiller')\n",
    "    student1_teacher1.compute_fidelity(test_loader)\n",
    "\n",
    "    student2_teacher2 = Distiller(student=mlp_student_2, teacher=teacher_2, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_2 + 'distiller')\n",
    "    student2_teacher2.compute_fidelity(test_loader)\n",
    "\n",
    "validate(model=student1_teacher1.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "validate(model=student2_teacher2.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "student2_teacher1 = Distiller(student=student2_teacher2.get_student(), teacher=teacher_1, device=device, lr=0.001)\n",
    "validate(model=student2_teacher1.get_student(), weights_file=None, valid_data=test_loader, device=device, is_mlp= True)\n",
    "student2_teacher1.compute_fidelity(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
