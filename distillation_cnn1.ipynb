{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1372\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'C:\\\\Users\\\\nicol\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\transformers\\\\utils'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#imports\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\convnext.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\ops\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_register_onnx_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register_custom_op\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboxes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     batched_nms,\n\u001b[0;32m      4\u001b[0m     box_area,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     remove_small_boxes,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mciou_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m complete_box_iou_loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\ops\\_register_onnx_ops.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m symbolic_opset11 \u001b[38;5;28;01mas\u001b[39;00m opset11\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_helper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_args\n\u001b[0;32m      8\u001b[0m _ONNX_OPSET_VERSION_11 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\onnx\\__init__.py:46\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckerError  \u001b[38;5;66;03m# Backwards compatibility\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     _optimize_graph,\n\u001b[0;32m     36\u001b[0m     _run_symbolic_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m     unregister_custom_op_symbolic,\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort:skip. needs to be last to avoid circular import\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     ExportOptions,\n\u001b[0;32m     48\u001b[0m     ExportOutput,\n\u001b[0;32m     49\u001b[0m     ExportOutputSerializer,\n\u001b[0;32m     50\u001b[0m     dynamo_export,\n\u001b[0;32m     51\u001b[0m     OnnxExporterError,\n\u001b[0;32m     52\u001b[0m     enable_fake_mode,\n\u001b[0;32m     53\u001b[0m     OnnxRegistry,\n\u001b[0;32m     54\u001b[0m     DiagnosticOptions,\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnxruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     58\u001b[0m     is_onnxrt_backend_supported,\n\u001b[0;32m     59\u001b[0m     OrtBackend \u001b[38;5;28;01mas\u001b[39;00m _OrtBackend,\n\u001b[0;32m     60\u001b[0m     OrtBackendOptions \u001b[38;5;28;01mas\u001b[39;00m _OrtBackendOptions,\n\u001b[0;32m     61\u001b[0m     OrtExecutionProvider \u001b[38;5;28;01mas\u001b[39;00m _OrtExecutionProvider,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Modules\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbolic_helper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_onnxrt_backend_supported\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    111\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\onnx\\_internal\\exporter.py:42\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _beartype, io_adapter\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infra\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     decomposition_table,\n\u001b[0;32m     44\u001b[0m     patcher \u001b[38;5;28;01mas\u001b[39;00m patcher,\n\u001b[0;32m     45\u001b[0m     registration,\n\u001b[0;32m     46\u001b[0m     serialization \u001b[38;5;28;01mas\u001b[39;00m fx_serialization,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# We can only import onnx from this module in a type-checking context to ensure that\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 'import torch.onnx' continues to work without having 'onnx' installed. We fully\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# 'import onnx' inside of dynamo_export (by way of _assert_dependencies).\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\onnx\\_internal\\fx\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ONNXTorchPatcher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model_with_external_data\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_model_with_external_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONNXTorchPatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\onnx\\_internal\\fx\\patcher.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# safetensors is not an exporter requirement, but needed for some huggingface models\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]  # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msafetensors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m safetensors_torch  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     has_safetensors_and_transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\utils\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     25\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     ContextManagers,\n\u001b[0;32m     33\u001b[0m     ExplicitEnum,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     60\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1408\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1374\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[1;34m(cls, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1350\u001b[0m, in \u001b[0;36m_path_hooks\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen zipimport>:76\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models.cnn import SimpleCNN\n",
    "from models.mlp import MLP\n",
    "from distillation_utils import Distiller\n",
    "from invariances_utils import shift_preserving_shape, test_IM\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "# Define a simple CNN architecture\n",
    "class ShiftInvariantCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShiftInvariantCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the MNIST dataset with data augmentation for training\n",
    "\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = ShiftInvariantCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "sicnn = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "class LeNet5(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self):   \n",
    "        super(LeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Convolution\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)\n",
    "        # Max-pooling\n",
    "        self.max_pool_2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(16*5*5, 120)   # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)       # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_1(x)\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # first flatten 'max_pool_2_out' to contain 16*5*5 columns\n",
    "        # read through https://stackoverflow.com/a/42482819/7551231\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LeNet5().to(\"cuda:0\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models.cnn import SimpleCNN\n",
    "from models.mlp import MLP\n",
    "from distillation_utils import Distiller\n",
    "from invariances_utils import shift_preserving_shape, test_IM\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "num_conv_layers = 2\n",
    "temperature = 1\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "TRAIN = False\n",
    "device = 'cuda'\n",
    "\n",
    "cnn = SimpleCNN(in_channels=in_channels, num_classes=num_classes, num_conv_layers=num_conv_layers, temperature=temperature).to('cuda:0')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      6\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(\"cuda:0\")\n",
    "        labels = labels.to(\"cuda:0\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "sicnn = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lenetSI/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (max_pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (max_pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sicnn = LeNet5()\n",
    "state_dict = torch.load(\"lenet/model\")\n",
    "sicnn.load_state_dict(state_dict)\n",
    "sicnn.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import antialiased_cnns\n",
    "\n",
    "class LeNetSI(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetSI, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(), antialiased_cnns.BlurPool(6, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), antialiased_cnns.BlurPool(6, stride=2), \n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(), antialiased_cnns.BlurPool(16, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1), antialiased_cnns.BlurPool(16, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = LeNetSI().to(\"cuda:0\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(\"cuda:0\")\n",
    "        labels = labels.to(\"cuda:0\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "sicnn = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Not using softmax\n",
      "Correct normal: 0.9684\n",
      "Correct shifted: 0.2838\n",
      "Correct cnn normal: 0.9909\n",
      "Correct cnn shifted: 0.6558\n",
      "\n",
      "tensor(0.4761, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9915, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models.cnn import SimpleCNN\n",
    "from models.mlp import MLP\n",
    "from distillation_utils import Distiller\n",
    "from invariances_utils import shift_preserving_shape, test_IM\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "num_conv_layers = 2\n",
    "temperature = 1\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "TRAIN = False\n",
    "device = 'cuda'\n",
    "#np.random.seed(42)\n",
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#loading distilled MLP\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "          hidden_layers= 4, device='cuda')\n",
    "cnn = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "          hidden_layers= 4, device='cuda')\n",
    "cnn_path = \"saved_models/model\"\n",
    "cnn = SimpleCNN(in_channels=in_channels, num_classes=num_classes, num_conv_layers=num_conv_layers, temperature=temperature).to('cuda:0')\n",
    "distiller = Distiller(student=mlp_student, teacher=cnn, device='cuda', lr=0.001)\n",
    "state_dict = torch.load(cnn_path)\n",
    "#cnn.load_state_dict(state_dict=state_dict)\n",
    "cnn = sicnn\n",
    "if TRAIN:\n",
    "    distiller.distill(train_loader, test_loader)\n",
    "    torch.save(distiller.get_student().state_dict(), \"newdistillmethod/distiller\")  \n",
    "else:\n",
    "    state_dict = torch.load(\"newdistillmethod/distiller\")\n",
    "    distiller.get_student().load_state_dict(state_dict=state_dict)\n",
    "    #distiller.eval_student(train_loader)\n",
    "test_IM(test_loader, distiller.get_student(), cnn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "num_classes = 10\n",
    "num_conv_layers = 2\n",
    "temperature = 1\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "TRAIN = False\n",
    "device = 'cuda'\n",
    "#np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "#Obtaining CNN\n",
    "cnn_path = \"saved_models/model\"\n",
    "cnn = SimpleCNN(in_channels=in_channels, num_classes=num_classes, num_conv_layers=num_conv_layers, temperature=temperature).to('cuda:0')\n",
    "if TRAIN:\n",
    "    criterion_cnn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "    # model training\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            outputs = cnn(images.to('cuda'))\n",
    "            loss = criterion_cnn(outputs, labels.to('cuda'))\n",
    "\n",
    "            optimizer_cnn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_cnn.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    # Save the trained model\n",
    "    torch.save(cnn.state_dict(), cnn_path)\n",
    "    print(f\"Model saved as {cnn_path}!\")\n",
    "if not TRAIN:\n",
    "    state_dict = torch.load(cnn_path)\n",
    "    cnn.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "# Testing the model\n",
    "cnn.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = cnn(images.to('cuda'))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.to('cuda')).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.eval() got an unexpected keyword argument 'test_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TRAIN:\n\u001b[0;32m     10\u001b[0m     mlp \u001b[38;5;241m=\u001b[39m MLP(input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m784\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m num_classes, hidden_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m     11\u001b[0m             hidden_layers\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, from_saved_state_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Module.eval() got an unexpected keyword argument 'test_loader'"
     ]
    }
   ],
   "source": [
    "#Loading undistilled MLP\n",
    "if TRAIN:\n",
    "    mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "        hidden_layers= 4, device='cuda')\n",
    "    criterion_mlp = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_mlp = torch.optim.Adam(mlp.parameters(), lr=lr)\n",
    "    mlp.train(train_loader=train_loader, optimizer=optimizer_mlp, criterion=criterion_mlp, \n",
    "              num_epochs=5)\n",
    "if not TRAIN:\n",
    "    mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models/mlp\")\n",
    "mlp.eval(test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n"
     ]
    }
   ],
   "source": [
    "#loading distilled MLP\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "          hidden_layers= 4, device='cuda')\n",
    "distiller = Distiller(student=mlp_student, teacher=cnn, device='cuda', lr=0.001)\n",
    "if TRAIN:\n",
    "    distiller.distill(train_loader, test_loader)\n",
    "    torch.save(distiller.get_student().state_dict(), \"newdistillmethod/distiller\")  \n",
    "else:\n",
    "    state_dict = torch.load(\"newdistillmethod/distiller\")\n",
    "    distiller.get_student().load_state_dict(state_dict=state_dict)\n",
    "    #distiller.eval_student(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_IM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistiller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_student\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicol\\OneDrive - Politecnico di Milano\\Desktop\\eth\\distillinginvariances\\invariances_utils.py:131\u001b[0m, in \u001b[0;36mtest_IM\u001b[1;34m(loader, model)\u001b[0m\n\u001b[0;32m    129\u001b[0m     unshifted_labels \u001b[38;5;241m=\u001b[39m model(non_shifted)\n\u001b[0;32m    130\u001b[0m     shifted_labels \u001b[38;5;241m=\u001b[39m model(shifted)\n\u001b[1;32m--> 131\u001b[0m correct_shifted \u001b[38;5;241m=\u001b[39m correct_shifted \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43munshifted_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    132\u001b[0m correct_normal \u001b[38;5;241m=\u001b[39m correct_normal \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mmax(shifted_labels, dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    133\u001b[0m invariance_measures\u001b[38;5;241m.\u001b[39mappend(invariance_measure(unshifted_labels, shifted_labels)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "test_IM(test_loader, distiller.get_student())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "FABIAN TO DO\n",
    "----------------------\n",
    "METRICS:\n",
    "- test agreement metrics \n",
    "- indipendent metrics (ECE, NLL, topk) -> consistent with the literature \n",
    "we need to add to the eval process these metrics + have a table of consistence with the literature (what are those metrics for a good mlp/cnn/distilled mlp)\n",
    "EXPERIMENTS\n",
    "- Train 2 independent students with the same teacher, you compare the fidelities, if the 2 students have comparable fidelities they agree with the teacher because they generalize well \n",
    "train 2 students as the above cell + compute agreement metrics\n",
    "- test distillation with different temperatures: fix temperatures 1 4 8 16\n",
    "- different MLP model size + test that the mlp is lower in flops\n",
    "------------------------\n",
    "ALREADY DONE:\n",
    "- compute agreement metrics: how much has the student learnt to predict in the same way as the teacher. \n",
    "compute_agreement(student_model, teacher_model) - most of this code already written by Patrick: deepsets/test\n",
    "_____________________________________\n",
    "4 exp 4 temps\n",
    "self dist\n",
    "self dist shifted\n",
    "mlp vanilla\n",
    "cnn vanilla\n",
    "cnn mlp \n",
    "cnn stupider + mlp\n",
    "fidelity of mlp to t' cross fidelity wrt first cnn - 1 plot\n",
    "\n",
    "accuracy NLL ECE top1 agreement between teacher and student, KL divergence, invariance metric (crossentropy?) -> show patrick it's better\n",
    "\n",
    "\n",
    "NICOLE TO DO:\n",
    "---------------\n",
    "- try with bigger filter and bigger max pooling\n",
    "- change IM metric\n",
    "\n",
    "- rerun all experiments and make pretty plots\n",
    "- large, heavy, regularized MLP - create a large, heavy, regularized MLP and distill with that\n",
    "- scale: extended MNIST CIFAR-10 (but cumbersome)\n",
    "- uncertainties : different seeds \n",
    "- 42 101 121 240 308 random seeds \n",
    "- non shifted training set + mlp, mlp with distillation on a cnn, run those over test dataset that's shifted -> expect better performance on mlp dist on cnn\n",
    "- give shifted validation set performance (validation set loss)\n",
    "- test val set accuracy with alpha different\n",
    "\n",
    " \n",
    "TO DO TOMORROW:\n",
    "- understand the softmax thing\n",
    "- plotting with different temperatures\n",
    "\n",
    "We need to have consistent:\n",
    "- training\n",
    "- distillation\n",
    "- indipendent metrics (ECE, NLL, topk) -> consistent with the literature \n",
    "- fidelity metrics\n",
    "----------------------------\n",
    "- self distilling mlp\n",
    "- fix the distillation loss problem\n",
    "- distill an mlp over an mlp\n",
    "- train an mlp independently on shifted data - compares with unshifted but distilled \n",
    "------------------------------------------\n",
    "\n",
    "\n",
    "the model actually learns invariances through the teacher -> all of these results hold\n",
    "\n",
    "\n",
    "ECE,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization Issues:\n",
    "Incorrect or inconsistent normalization of input features can cause training instability. Ensure that your input data is properly normalized.\n",
    "\n",
    "Complexity of the Model:\n",
    "If your model is too complex for the given task or if it has too many parameters, it may struggle to generalize well, leading to overfitting and erratic loss behavior. Consider simplifying the model architecture or using regularization techniques.\n",
    "\n",
    "Vanishing or Exploding Gradients:\n",
    "Problems like vanishing or exploding gradients can hinder the convergence of the model. Apply techniques such as gradient clipping, weight regularization, or normalization layers (e.g., Batch Normalization) to address these issues.\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "Learning Rate Too High:\n",
    "If the learning rate is set too high, the optimization algorithm might overshoot the minimum, leading to oscillations or divergence. Try reducing the learning rate and observe how it affects the training.\n",
    "\n",
    "Learning Rate Schedule:\n",
    "Sometimes, using a learning rate schedule or adaptive learning rate methods (such as learning rate annealing) can help stabilize the training process. These methods adjust the learning rate during training to improve convergence.\n",
    "\n",
    "Poor Initialization: \n",
    "The initial weights of the neural network can have a significant impact on training. Poor weight initialization may lead to difficulties in finding a good solution. Try using appropriate weight initialization techniques.\n",
    "\n",
    "Batch Size:\n",
    "The choice of batch size can also affect the stability of training. Smaller batch sizes may introduce more variability in the gradients, leading to fluctuations in the loss.\n",
    "\n",
    "Data Issues:\n",
    "Check the quality and consistency of your training data. Noisy or inconsistent data can make it challenging for the model to learn a meaningful representation.\n",
    "\n",
    "Early Stopping:\n",
    "If the loss does not improve over a certain number of epochs, it might be worth considering early stopping. Monitor the validation loss and stop training if it starts to increase consistently.\n",
    "\n",
    "Monitor Metrics Beyond Loss:\n",
    "Loss is just one metric, and it might not always reflect the performance of the model accurately. Monitor other metrics, such as accuracy or validation performance, to get a more comprehensive view of the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Test Accuracy: 0.9612\n",
      "tensor(0.8725, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Not using softmax\n",
      "tensor(0.8738, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "hfafu\n",
      "Epoch [1/5], Step [100/938], Student Loss : 0.1635, Total Loss: 0.0442\n",
      "Epoch [1/5], Step [200/938], Student Loss : 0.0570, Total Loss: 0.0225\n",
      "Epoch [1/5], Step [300/938], Student Loss : 0.3554, Total Loss: 0.1328\n",
      "Epoch [1/5], Step [400/938], Student Loss : 0.1317, Total Loss: 0.1042\n",
      "Epoch [1/5], Step [500/938], Student Loss : 0.2529, Total Loss: 0.0886\n",
      "Epoch [1/5], Step [600/938], Student Loss : 0.0233, Total Loss: 0.0219\n",
      "Epoch [1/5], Step [700/938], Student Loss : 0.3350, Total Loss: 0.2302\n",
      "Epoch [1/5], Step [800/938], Student Loss : 0.0820, Total Loss: 0.0284\n",
      "Epoch [1/5], Step [900/938], Student Loss : 0.4317, Total Loss: 0.0910\n",
      "Epoch [2/5], Step [100/938], Student Loss : 0.1460, Total Loss: 0.3145\n",
      "Epoch [2/5], Step [200/938], Student Loss : 0.6121, Total Loss: 0.2191\n",
      "Epoch [2/5], Step [300/938], Student Loss : 0.1448, Total Loss: 0.0714\n",
      "Epoch [2/5], Step [400/938], Student Loss : 0.3349, Total Loss: 0.0413\n",
      "Epoch [2/5], Step [500/938], Student Loss : 0.2626, Total Loss: 0.0921\n",
      "Epoch [2/5], Step [600/938], Student Loss : 0.1225, Total Loss: 0.0426\n",
      "Epoch [2/5], Step [700/938], Student Loss : 0.2021, Total Loss: 0.0963\n",
      "Epoch [2/5], Step [800/938], Student Loss : 0.4059, Total Loss: 0.1832\n",
      "Epoch [2/5], Step [900/938], Student Loss : 0.6294, Total Loss: 0.0494\n",
      "Epoch [3/5], Step [100/938], Student Loss : 0.0622, Total Loss: 0.0631\n",
      "Epoch [3/5], Step [200/938], Student Loss : 0.1702, Total Loss: 0.1661\n",
      "Epoch [3/5], Step [300/938], Student Loss : 0.7188, Total Loss: 0.0331\n",
      "Epoch [3/5], Step [400/938], Student Loss : 0.2593, Total Loss: 0.1580\n",
      "Epoch [3/5], Step [500/938], Student Loss : 0.0707, Total Loss: 0.0276\n",
      "Epoch [3/5], Step [600/938], Student Loss : 0.0142, Total Loss: 0.0164\n",
      "Epoch [3/5], Step [700/938], Student Loss : 0.0015, Total Loss: 0.0054\n",
      "Epoch [3/5], Step [800/938], Student Loss : 0.2647, Total Loss: 0.0304\n",
      "Epoch [3/5], Step [900/938], Student Loss : 0.0132, Total Loss: 0.0099\n",
      "Epoch [4/5], Step [100/938], Student Loss : 0.1007, Total Loss: 0.1005\n",
      "Epoch [4/5], Step [200/938], Student Loss : 0.1186, Total Loss: 0.0490\n",
      "Epoch [4/5], Step [300/938], Student Loss : 0.3508, Total Loss: 0.0401\n",
      "Epoch [4/5], Step [400/938], Student Loss : 0.6503, Total Loss: 0.0766\n",
      "Epoch [4/5], Step [500/938], Student Loss : 0.2393, Total Loss: 0.0492\n",
      "Epoch [4/5], Step [600/938], Student Loss : 0.1205, Total Loss: 0.0840\n",
      "Epoch [4/5], Step [700/938], Student Loss : 0.0358, Total Loss: 0.0221\n",
      "Epoch [4/5], Step [800/938], Student Loss : 1.2560, Total Loss: 0.1168\n",
      "Epoch [4/5], Step [900/938], Student Loss : 0.0926, Total Loss: 0.1000\n",
      "Epoch [5/5], Step [100/938], Student Loss : 0.0374, Total Loss: 0.0186\n",
      "Epoch [5/5], Step [200/938], Student Loss : 0.1427, Total Loss: 0.1758\n",
      "Epoch [5/5], Step [300/938], Student Loss : 0.1114, Total Loss: 0.0248\n",
      "Epoch [5/5], Step [400/938], Student Loss : 0.0207, Total Loss: 0.0445\n",
      "Epoch [5/5], Step [500/938], Student Loss : 0.2438, Total Loss: 0.0220\n",
      "Epoch [5/5], Step [600/938], Student Loss : 0.2129, Total Loss: 0.1495\n",
      "Epoch [5/5], Step [700/938], Student Loss : 0.3182, Total Loss: 0.0395\n",
      "Epoch [5/5], Step [800/938], Student Loss : 0.2660, Total Loss: 0.0610\n",
      "Epoch [5/5], Step [900/938], Student Loss : 0.0518, Total Loss: 0.0222\n",
      "Student loss: 0.23638089361807538\n",
      "Distillation loss: 0.07954516015532943\n",
      "Total loss: 0.07954516015532943\n",
      "saved model\n",
      "Test Accuracy: 0.9566\n"
     ]
    }
   ],
   "source": [
    "#Self distilling MLP (only from loaded data)\n",
    "\n",
    "#Self distillation: mlp_student and mlp teacher coincide #TODO CHECK\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models/mlp\")\n",
    "mlp_student.eval(test_loader)\n",
    "print(test_IM(test_loader, mlp_student))\n",
    "\n",
    "mlp_teacher = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models/mlp\")\n",
    "print(test_IM(test_loader, mlp_teacher))\n",
    "\n",
    "if TRAIN:\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device='cuda', lr=0.001)\n",
    "    selfdistiller.distill(train_loader, 5, \"saved_models_selfdistill/\")\n",
    "    selfdistiller.test_step(test_loader=test_loader)\n",
    "\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device='cuda', lr=0.001,\n",
    "                        load_student_from_path = 'saved_models_selfdistill/distiller')\n",
    "    selfdistiller.test_step(test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8889, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_IM(test_loader, selfdistiller.get_student())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Not using softmax\n",
      "tensor(0.8729, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "hfafu\n",
      "Epoch [1/5], Step [100/938], Student Loss : 27.8136, Total Loss: 8.6661\n",
      "Epoch [1/5], Step [200/938], Student Loss : 188.4176, Total Loss: 11.5220\n",
      "Epoch [1/5], Step [300/938], Student Loss : 662.5803, Total Loss: 13.8150\n",
      "Epoch [1/5], Step [400/938], Student Loss : 752.3962, Total Loss: 14.6692\n",
      "Epoch [1/5], Step [500/938], Student Loss : 609.8974, Total Loss: 14.1995\n",
      "Epoch [1/5], Step [600/938], Student Loss : 2639.6240, Total Loss: 15.4462\n",
      "Epoch [1/5], Step [700/938], Student Loss : 2322.1643, Total Loss: 14.6260\n",
      "Epoch [1/5], Step [800/938], Student Loss : 2393.5117, Total Loss: 14.2670\n",
      "Epoch [1/5], Step [900/938], Student Loss : 2528.3132, Total Loss: 17.2357\n",
      "Epoch [2/5], Step [100/938], Student Loss : 2206.9958, Total Loss: 14.5140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN:\n\u001b[0;32m     10\u001b[0m     mlp_mlp_distiller \u001b[38;5;241m=\u001b[39m Distiller(student\u001b[38;5;241m=\u001b[39mmlp_student, teacher\u001b[38;5;241m=\u001b[39mmlp_teacher, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mmlp_mlp_distiller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaved_models_mlpfrommlp/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     mlp_mlp_distiller\u001b[38;5;241m.\u001b[39mtest_step(test_loader\u001b[38;5;241m=\u001b[39mtest_loader)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TRAIN:\n",
      "File \u001b[1;32mc:\\Users\\nicol\\OneDrive - Politecnico di Milano\\Desktop\\eth\\distillinginvariances\\distillation_utils.py:41\u001b[0m, in \u001b[0;36mDistiller.distill\u001b[1;34m(self, train_dataloader, epochs, save_path_folder)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Train the student network through one feed forward.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y)  \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     43\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Distilling MLP from MLP\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda')\n",
    "\n",
    "mlp_teacher = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models/mlp\")\n",
    "print(test_IM(test_loader, mlp_teacher))\n",
    "\n",
    "if TRAIN:\n",
    "    mlp_mlp_distiller = Distiller(student=mlp_student, teacher=mlp_teacher, device='cuda', lr=0.001)\n",
    "    mlp_mlp_distiller.distill(train_loader, 5, \"saved_models_mlpfrommlp/\")\n",
    "    mlp_mlp_distiller.test_step(test_loader=test_loader)\n",
    "\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    mlp_mlp_distiller = Distiller(student=mlp_student, teacher=mlp_teacher, device='cuda', lr=0.001,\n",
    "                        load_student_from_path = 'saved_models_mlpfrommlp/distiller')\n",
    "    mlp_mlp_distiller.test_step(test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1270, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_IM(test_loader, mlp_mlp_distiller.get_student())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset that combines MNIST and additional data\n",
    "class ShiftAugmentedMNIST(Dataset):\n",
    "    def __init__(self, mnist_dataset, translation_times : int = 5, max_shift : int = 7):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        directions = [\"u\",\"d\",\"l\",\"r\"]\n",
    "        self.translations = []\n",
    "        for i in range(len(self.mnist_dataset)):\n",
    "            img, label = self.mnist_dataset[i]\n",
    "            img = img.squeeze()\n",
    "            for t in range(translation_times):\n",
    "                sh = shift_preserving_shape(img, direction=directions[np.random.randint(0,4)],\n",
    "                                            max_shift=max_shift).unsqueeze(0)\n",
    "                if sh is not None:\n",
    "                    self.translations.append((sh, label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.mnist_dataset):\n",
    "            return self.mnist_dataset[index]\n",
    "        else:\n",
    "            return self.translations[index - len(self.mnist_dataset)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset) + len(self.translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_augmented_dataset = ShiftAugmentedMNIST(train_dataset)\n",
    "train_augmented_loader = DataLoader(dataset=train_augmented_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Epoch [1/5], Step [100/5625], Loss: 1.4332\n",
      "Epoch [1/5], Step [200/5625], Loss: 1.2715\n",
      "Epoch [1/5], Step [300/5625], Loss: 0.9806\n",
      "Epoch [1/5], Step [400/5625], Loss: 0.6599\n",
      "Epoch [1/5], Step [500/5625], Loss: 0.4850\n",
      "Epoch [1/5], Step [600/5625], Loss: 0.6876\n",
      "Epoch [1/5], Step [700/5625], Loss: 0.3734\n",
      "Epoch [1/5], Step [800/5625], Loss: 0.4117\n",
      "Epoch [1/5], Step [900/5625], Loss: 0.5502\n",
      "Epoch [1/5], Step [1000/5625], Loss: 0.3068\n",
      "Epoch [1/5], Step [1100/5625], Loss: 0.3532\n",
      "Epoch [1/5], Step [1200/5625], Loss: 0.3167\n",
      "Epoch [1/5], Step [1300/5625], Loss: 0.2602\n",
      "Epoch [1/5], Step [1400/5625], Loss: 0.1548\n",
      "Epoch [1/5], Step [1500/5625], Loss: 0.3497\n",
      "Epoch [1/5], Step [1600/5625], Loss: 0.3821\n",
      "Epoch [1/5], Step [1700/5625], Loss: 0.3323\n",
      "Epoch [1/5], Step [1800/5625], Loss: 0.1482\n",
      "Epoch [1/5], Step [1900/5625], Loss: 0.4713\n",
      "Epoch [1/5], Step [2000/5625], Loss: 0.2970\n",
      "Epoch [1/5], Step [2100/5625], Loss: 0.5642\n",
      "Epoch [1/5], Step [2200/5625], Loss: 0.2986\n",
      "Epoch [1/5], Step [2300/5625], Loss: 0.2690\n",
      "Epoch [1/5], Step [2400/5625], Loss: 0.3755\n",
      "Epoch [1/5], Step [2500/5625], Loss: 0.2289\n",
      "Epoch [1/5], Step [2600/5625], Loss: 0.2848\n",
      "Epoch [1/5], Step [2700/5625], Loss: 0.1836\n",
      "Epoch [1/5], Step [2800/5625], Loss: 0.2984\n",
      "Epoch [1/5], Step [2900/5625], Loss: 0.2754\n",
      "Epoch [1/5], Step [3000/5625], Loss: 0.3038\n",
      "Epoch [1/5], Step [3100/5625], Loss: 0.1399\n",
      "Epoch [1/5], Step [3200/5625], Loss: 0.1599\n",
      "Epoch [1/5], Step [3300/5625], Loss: 0.1928\n",
      "Epoch [1/5], Step [3400/5625], Loss: 0.1422\n",
      "Epoch [1/5], Step [3500/5625], Loss: 0.2894\n",
      "Epoch [1/5], Step [3600/5625], Loss: 0.2790\n",
      "Epoch [1/5], Step [3700/5625], Loss: 0.1240\n",
      "Epoch [1/5], Step [3800/5625], Loss: 0.2227\n",
      "Epoch [1/5], Step [3900/5625], Loss: 0.2888\n",
      "Epoch [1/5], Step [4000/5625], Loss: 0.1220\n",
      "Epoch [1/5], Step [4100/5625], Loss: 0.2940\n",
      "Epoch [1/5], Step [4200/5625], Loss: 0.0622\n",
      "Epoch [1/5], Step [4300/5625], Loss: 0.1478\n",
      "Epoch [1/5], Step [4400/5625], Loss: 0.1204\n",
      "Epoch [1/5], Step [4500/5625], Loss: 0.2260\n",
      "Epoch [1/5], Step [4600/5625], Loss: 0.2048\n",
      "Epoch [1/5], Step [4700/5625], Loss: 0.2649\n",
      "Epoch [1/5], Step [4800/5625], Loss: 0.4703\n",
      "Epoch [1/5], Step [4900/5625], Loss: 0.0749\n",
      "Epoch [1/5], Step [5000/5625], Loss: 0.1190\n",
      "Epoch [1/5], Step [5100/5625], Loss: 0.1537\n",
      "Epoch [1/5], Step [5200/5625], Loss: 0.2951\n",
      "Epoch [1/5], Step [5300/5625], Loss: 0.1419\n",
      "Epoch [1/5], Step [5400/5625], Loss: 0.1271\n",
      "Epoch [1/5], Step [5500/5625], Loss: 0.1749\n",
      "Epoch [1/5], Step [5600/5625], Loss: 0.1750\n",
      "Epoch [2/5], Step [100/5625], Loss: 0.1902\n",
      "Epoch [2/5], Step [200/5625], Loss: 0.1419\n",
      "Epoch [2/5], Step [300/5625], Loss: 0.0704\n",
      "Epoch [2/5], Step [400/5625], Loss: 0.0641\n",
      "Epoch [2/5], Step [500/5625], Loss: 0.1618\n",
      "Epoch [2/5], Step [600/5625], Loss: 0.1698\n",
      "Epoch [2/5], Step [700/5625], Loss: 0.0382\n",
      "Epoch [2/5], Step [800/5625], Loss: 0.1359\n",
      "Epoch [2/5], Step [900/5625], Loss: 0.3849\n",
      "Epoch [2/5], Step [1000/5625], Loss: 0.0446\n",
      "Epoch [2/5], Step [1100/5625], Loss: 0.0235\n",
      "Epoch [2/5], Step [1200/5625], Loss: 0.2652\n",
      "Epoch [2/5], Step [1300/5625], Loss: 0.0421\n",
      "Epoch [2/5], Step [1400/5625], Loss: 0.1173\n",
      "Epoch [2/5], Step [1500/5625], Loss: 0.0259\n",
      "Epoch [2/5], Step [1600/5625], Loss: 0.1037\n",
      "Epoch [2/5], Step [1700/5625], Loss: 0.0819\n",
      "Epoch [2/5], Step [1800/5625], Loss: 0.0990\n",
      "Epoch [2/5], Step [1900/5625], Loss: 0.1472\n",
      "Epoch [2/5], Step [2000/5625], Loss: 0.2541\n",
      "Epoch [2/5], Step [2100/5625], Loss: 0.2973\n",
      "Epoch [2/5], Step [2200/5625], Loss: 0.2275\n",
      "Epoch [2/5], Step [2300/5625], Loss: 0.1974\n",
      "Epoch [2/5], Step [2400/5625], Loss: 0.0856\n",
      "Epoch [2/5], Step [2500/5625], Loss: 0.0307\n",
      "Epoch [2/5], Step [2600/5625], Loss: 0.1929\n",
      "Epoch [2/5], Step [2700/5625], Loss: 0.1274\n",
      "Epoch [2/5], Step [2800/5625], Loss: 0.1319\n",
      "Epoch [2/5], Step [2900/5625], Loss: 0.0701\n",
      "Epoch [2/5], Step [3000/5625], Loss: 0.0948\n",
      "Epoch [2/5], Step [3100/5625], Loss: 0.1120\n",
      "Epoch [2/5], Step [3200/5625], Loss: 0.0588\n",
      "Epoch [2/5], Step [3300/5625], Loss: 0.0761\n",
      "Epoch [2/5], Step [3400/5625], Loss: 0.1612\n",
      "Epoch [2/5], Step [3500/5625], Loss: 0.1310\n",
      "Epoch [2/5], Step [3600/5625], Loss: 0.1614\n",
      "Epoch [2/5], Step [3700/5625], Loss: 0.1081\n",
      "Epoch [2/5], Step [3800/5625], Loss: 0.0227\n",
      "Epoch [2/5], Step [3900/5625], Loss: 0.2509\n",
      "Epoch [2/5], Step [4000/5625], Loss: 0.0510\n",
      "Epoch [2/5], Step [4100/5625], Loss: 0.1035\n",
      "Epoch [2/5], Step [4200/5625], Loss: 0.0974\n",
      "Epoch [2/5], Step [4300/5625], Loss: 0.0959\n",
      "Epoch [2/5], Step [4400/5625], Loss: 0.1273\n",
      "Epoch [2/5], Step [4500/5625], Loss: 0.3123\n",
      "Epoch [2/5], Step [4600/5625], Loss: 0.2246\n",
      "Epoch [2/5], Step [4700/5625], Loss: 0.0820\n",
      "Epoch [2/5], Step [4800/5625], Loss: 0.2338\n",
      "Epoch [2/5], Step [4900/5625], Loss: 0.1395\n",
      "Epoch [2/5], Step [5000/5625], Loss: 0.0950\n",
      "Epoch [2/5], Step [5100/5625], Loss: 0.1264\n",
      "Epoch [2/5], Step [5200/5625], Loss: 0.2361\n",
      "Epoch [2/5], Step [5300/5625], Loss: 0.1898\n",
      "Epoch [2/5], Step [5400/5625], Loss: 0.1489\n",
      "Epoch [2/5], Step [5500/5625], Loss: 0.3006\n",
      "Epoch [2/5], Step [5600/5625], Loss: 0.1444\n",
      "Epoch [3/5], Step [100/5625], Loss: 0.0385\n",
      "Epoch [3/5], Step [200/5625], Loss: 0.1098\n",
      "Epoch [3/5], Step [300/5625], Loss: 0.0625\n",
      "Epoch [3/5], Step [400/5625], Loss: 0.0834\n",
      "Epoch [3/5], Step [500/5625], Loss: 0.0394\n",
      "Epoch [3/5], Step [600/5625], Loss: 0.1651\n",
      "Epoch [3/5], Step [700/5625], Loss: 0.2128\n",
      "Epoch [3/5], Step [800/5625], Loss: 0.1148\n",
      "Epoch [3/5], Step [900/5625], Loss: 0.1091\n",
      "Epoch [3/5], Step [1000/5625], Loss: 0.0945\n",
      "Epoch [3/5], Step [1100/5625], Loss: 0.2285\n",
      "Epoch [3/5], Step [1200/5625], Loss: 0.1825\n",
      "Epoch [3/5], Step [1300/5625], Loss: 0.1352\n",
      "Epoch [3/5], Step [1400/5625], Loss: 0.1150\n",
      "Epoch [3/5], Step [1500/5625], Loss: 0.1534\n",
      "Epoch [3/5], Step [1600/5625], Loss: 0.0674\n",
      "Epoch [3/5], Step [1700/5625], Loss: 0.1360\n",
      "Epoch [3/5], Step [1800/5625], Loss: 0.0457\n",
      "Epoch [3/5], Step [1900/5625], Loss: 0.0906\n",
      "Epoch [3/5], Step [2000/5625], Loss: 0.1029\n",
      "Epoch [3/5], Step [2100/5625], Loss: 0.0951\n",
      "Epoch [3/5], Step [2200/5625], Loss: 0.0820\n",
      "Epoch [3/5], Step [2300/5625], Loss: 0.0861\n",
      "Epoch [3/5], Step [2400/5625], Loss: 0.0868\n",
      "Epoch [3/5], Step [2500/5625], Loss: 0.0669\n",
      "Epoch [3/5], Step [2600/5625], Loss: 0.0570\n",
      "Epoch [3/5], Step [2700/5625], Loss: 0.1801\n",
      "Epoch [3/5], Step [2800/5625], Loss: 0.0111\n",
      "Epoch [3/5], Step [2900/5625], Loss: 0.0201\n",
      "Epoch [3/5], Step [3000/5625], Loss: 0.0358\n",
      "Epoch [3/5], Step [3100/5625], Loss: 0.0204\n",
      "Epoch [3/5], Step [3200/5625], Loss: 0.0621\n",
      "Epoch [3/5], Step [3300/5625], Loss: 0.0249\n",
      "Epoch [3/5], Step [3400/5625], Loss: 0.0287\n",
      "Epoch [3/5], Step [3500/5625], Loss: 0.0712\n",
      "Epoch [3/5], Step [3600/5625], Loss: 0.0772\n",
      "Epoch [3/5], Step [3700/5625], Loss: 0.0530\n",
      "Epoch [3/5], Step [3800/5625], Loss: 0.0392\n",
      "Epoch [3/5], Step [3900/5625], Loss: 0.0605\n",
      "Epoch [3/5], Step [4000/5625], Loss: 0.0673\n",
      "Epoch [3/5], Step [4100/5625], Loss: 0.1655\n",
      "Epoch [3/5], Step [4200/5625], Loss: 0.0780\n",
      "Epoch [3/5], Step [4300/5625], Loss: 0.0322\n",
      "Epoch [3/5], Step [4400/5625], Loss: 0.0845\n",
      "Epoch [3/5], Step [4500/5625], Loss: 0.2161\n",
      "Epoch [3/5], Step [4600/5625], Loss: 0.1500\n",
      "Epoch [3/5], Step [4700/5625], Loss: 0.0963\n",
      "Epoch [3/5], Step [4800/5625], Loss: 0.0807\n",
      "Epoch [3/5], Step [4900/5625], Loss: 0.0329\n",
      "Epoch [3/5], Step [5000/5625], Loss: 0.1709\n",
      "Epoch [3/5], Step [5100/5625], Loss: 0.0387\n",
      "Epoch [3/5], Step [5200/5625], Loss: 0.1502\n",
      "Epoch [3/5], Step [5300/5625], Loss: 0.0879\n",
      "Epoch [3/5], Step [5400/5625], Loss: 0.0125\n",
      "Epoch [3/5], Step [5500/5625], Loss: 0.1266\n",
      "Epoch [3/5], Step [5600/5625], Loss: 0.1098\n",
      "Epoch [4/5], Step [100/5625], Loss: 0.3617\n",
      "Epoch [4/5], Step [200/5625], Loss: 0.0429\n",
      "Epoch [4/5], Step [300/5625], Loss: 0.0694\n",
      "Epoch [4/5], Step [400/5625], Loss: 0.0576\n",
      "Epoch [4/5], Step [500/5625], Loss: 0.0754\n",
      "Epoch [4/5], Step [600/5625], Loss: 0.0178\n",
      "Epoch [4/5], Step [700/5625], Loss: 0.0123\n",
      "Epoch [4/5], Step [800/5625], Loss: 0.0472\n",
      "Epoch [4/5], Step [900/5625], Loss: 0.0718\n",
      "Epoch [4/5], Step [1000/5625], Loss: 0.0730\n",
      "Epoch [4/5], Step [1100/5625], Loss: 0.0074\n",
      "Epoch [4/5], Step [1200/5625], Loss: 0.0466\n",
      "Epoch [4/5], Step [1300/5625], Loss: 0.1129\n",
      "Epoch [4/5], Step [1400/5625], Loss: 0.0427\n",
      "Epoch [4/5], Step [1500/5625], Loss: 0.0090\n",
      "Epoch [4/5], Step [1600/5625], Loss: 0.0764\n",
      "Epoch [4/5], Step [1700/5625], Loss: 0.0565\n",
      "Epoch [4/5], Step [1800/5625], Loss: 0.0590\n",
      "Epoch [4/5], Step [1900/5625], Loss: 0.0116\n",
      "Epoch [4/5], Step [2000/5625], Loss: 0.1379\n",
      "Epoch [4/5], Step [2100/5625], Loss: 0.0465\n",
      "Epoch [4/5], Step [2200/5625], Loss: 0.0853\n",
      "Epoch [4/5], Step [2300/5625], Loss: 0.0458\n",
      "Epoch [4/5], Step [2400/5625], Loss: 0.1168\n",
      "Epoch [4/5], Step [2500/5625], Loss: 0.0599\n",
      "Epoch [4/5], Step [2600/5625], Loss: 0.0146\n",
      "Epoch [4/5], Step [2700/5625], Loss: 0.1221\n",
      "Epoch [4/5], Step [2800/5625], Loss: 0.0301\n",
      "Epoch [4/5], Step [2900/5625], Loss: 0.0358\n",
      "Epoch [4/5], Step [3000/5625], Loss: 0.0825\n",
      "Epoch [4/5], Step [3100/5625], Loss: 0.0325\n",
      "Epoch [4/5], Step [3200/5625], Loss: 0.1023\n",
      "Epoch [4/5], Step [3300/5625], Loss: 0.0089\n",
      "Epoch [4/5], Step [3400/5625], Loss: 0.0792\n",
      "Epoch [4/5], Step [3500/5625], Loss: 0.1531\n",
      "Epoch [4/5], Step [3600/5625], Loss: 0.0479\n",
      "Epoch [4/5], Step [3700/5625], Loss: 0.0164\n",
      "Epoch [4/5], Step [3800/5625], Loss: 0.1381\n",
      "Epoch [4/5], Step [3900/5625], Loss: 0.1549\n",
      "Epoch [4/5], Step [4000/5625], Loss: 0.0111\n",
      "Epoch [4/5], Step [4100/5625], Loss: 0.0709\n",
      "Epoch [4/5], Step [4200/5625], Loss: 0.0313\n",
      "Epoch [4/5], Step [4300/5625], Loss: 0.1134\n",
      "Epoch [4/5], Step [4400/5625], Loss: 0.0575\n",
      "Epoch [4/5], Step [4500/5625], Loss: 0.0645\n",
      "Epoch [4/5], Step [4600/5625], Loss: 0.0250\n",
      "Epoch [4/5], Step [4700/5625], Loss: 0.0376\n",
      "Epoch [4/5], Step [4800/5625], Loss: 0.0265\n",
      "Epoch [4/5], Step [4900/5625], Loss: 0.1111\n",
      "Epoch [4/5], Step [5000/5625], Loss: 0.0714\n",
      "Epoch [4/5], Step [5100/5625], Loss: 0.1223\n",
      "Epoch [4/5], Step [5200/5625], Loss: 0.0550\n",
      "Epoch [4/5], Step [5300/5625], Loss: 0.0902\n",
      "Epoch [4/5], Step [5400/5625], Loss: 0.1042\n",
      "Epoch [4/5], Step [5500/5625], Loss: 0.0648\n",
      "Epoch [4/5], Step [5600/5625], Loss: 0.0291\n",
      "Epoch [5/5], Step [100/5625], Loss: 0.1230\n",
      "Epoch [5/5], Step [200/5625], Loss: 0.1047\n",
      "Epoch [5/5], Step [300/5625], Loss: 0.1411\n",
      "Epoch [5/5], Step [400/5625], Loss: 0.0220\n",
      "Epoch [5/5], Step [500/5625], Loss: 0.0374\n",
      "Epoch [5/5], Step [600/5625], Loss: 0.0197\n",
      "Epoch [5/5], Step [700/5625], Loss: 0.1289\n",
      "Epoch [5/5], Step [800/5625], Loss: 0.0906\n",
      "Epoch [5/5], Step [900/5625], Loss: 0.0456\n",
      "Epoch [5/5], Step [1000/5625], Loss: 0.3653\n",
      "Epoch [5/5], Step [1100/5625], Loss: 0.0934\n",
      "Epoch [5/5], Step [1200/5625], Loss: 0.2809\n",
      "Epoch [5/5], Step [1300/5625], Loss: 0.0071\n",
      "Epoch [5/5], Step [1400/5625], Loss: 0.0495\n",
      "Epoch [5/5], Step [1500/5625], Loss: 0.0583\n",
      "Epoch [5/5], Step [1600/5625], Loss: 0.0198\n",
      "Epoch [5/5], Step [1700/5625], Loss: 0.0169\n",
      "Epoch [5/5], Step [1800/5625], Loss: 0.0540\n",
      "Epoch [5/5], Step [1900/5625], Loss: 0.0965\n",
      "Epoch [5/5], Step [2000/5625], Loss: 0.0915\n",
      "Epoch [5/5], Step [2100/5625], Loss: 0.1796\n",
      "Epoch [5/5], Step [2200/5625], Loss: 0.0630\n",
      "Epoch [5/5], Step [2300/5625], Loss: 0.0757\n",
      "Epoch [5/5], Step [2400/5625], Loss: 0.0468\n",
      "Epoch [5/5], Step [2500/5625], Loss: 0.0772\n",
      "Epoch [5/5], Step [2600/5625], Loss: 0.0301\n",
      "Epoch [5/5], Step [2700/5625], Loss: 0.0174\n",
      "Epoch [5/5], Step [2800/5625], Loss: 0.0153\n",
      "Epoch [5/5], Step [2900/5625], Loss: 0.1001\n",
      "Epoch [5/5], Step [3000/5625], Loss: 0.0535\n",
      "Epoch [5/5], Step [3100/5625], Loss: 0.1685\n",
      "Epoch [5/5], Step [3200/5625], Loss: 0.0222\n",
      "Epoch [5/5], Step [3300/5625], Loss: 0.0173\n",
      "Epoch [5/5], Step [3400/5625], Loss: 0.0425\n",
      "Epoch [5/5], Step [3500/5625], Loss: 0.0098\n",
      "Epoch [5/5], Step [3600/5625], Loss: 0.0074\n",
      "Epoch [5/5], Step [3700/5625], Loss: 0.0594\n",
      "Epoch [5/5], Step [3800/5625], Loss: 0.0444\n",
      "Epoch [5/5], Step [3900/5625], Loss: 0.0820\n",
      "Epoch [5/5], Step [4000/5625], Loss: 0.0174\n",
      "Epoch [5/5], Step [4100/5625], Loss: 0.1422\n",
      "Epoch [5/5], Step [4200/5625], Loss: 0.0812\n",
      "Epoch [5/5], Step [4300/5625], Loss: 0.1431\n",
      "Epoch [5/5], Step [4400/5625], Loss: 0.0715\n",
      "Epoch [5/5], Step [4500/5625], Loss: 0.1149\n",
      "Epoch [5/5], Step [4600/5625], Loss: 0.0418\n",
      "Epoch [5/5], Step [4700/5625], Loss: 0.0755\n",
      "Epoch [5/5], Step [4800/5625], Loss: 0.0267\n",
      "Epoch [5/5], Step [4900/5625], Loss: 0.0550\n",
      "Epoch [5/5], Step [5000/5625], Loss: 0.0158\n",
      "Epoch [5/5], Step [5100/5625], Loss: 0.1650\n",
      "Epoch [5/5], Step [5200/5625], Loss: 0.0563\n",
      "Epoch [5/5], Step [5300/5625], Loss: 0.0188\n",
      "Epoch [5/5], Step [5400/5625], Loss: 0.1015\n",
      "Epoch [5/5], Step [5500/5625], Loss: 0.0352\n",
      "Epoch [5/5], Step [5600/5625], Loss: 0.0120\n",
      "Model saved as saved_models_shiftinvariantmlp\\mlp!\n",
      "Test Accuracy: 0.9749\n"
     ]
    }
   ],
   "source": [
    "#Evaluating MLP trained on invariance data\n",
    "if TRAIN:\n",
    "    shift_invariant_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "        hidden_layers= 4, device='cuda')\n",
    "    criterion_mlp = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_mlp = torch.optim.Adam(shift_invariant_mlp.parameters(), lr=lr)\n",
    "    shift_invariant_mlp.train(train_loader=train_augmented_loader, optimizer=optimizer_mlp, criterion=criterion_mlp, \n",
    "              num_epochs=5, save_path_folder = \"saved_models_shiftinvariantmlp\")\n",
    "if not TRAIN:\n",
    "    shift_invariant_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models_shiftinvariantmlp/mlp\")\n",
    "shift_invariant_mlp.eval(test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0822, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_IM(test_loader, shift_invariant_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Not using softmax\n",
      "Invariance of teacher:tensor(0.0821, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch [1/5], Step [100/938], Student Loss : 3.4604, Total Loss: 15.6608\n",
      "Epoch [1/5], Step [200/938], Student Loss : 2.1550, Total Loss: 3.0418\n",
      "Epoch [1/5], Step [300/938], Student Loss : 0.2299, Total Loss: 1.6676\n",
      "Epoch [1/5], Step [400/938], Student Loss : 0.9416, Total Loss: 2.1623\n",
      "Epoch [1/5], Step [500/938], Student Loss : 1.0407, Total Loss: 3.0659\n",
      "Epoch [1/5], Step [600/938], Student Loss : 0.8709, Total Loss: 2.0034\n",
      "Epoch [1/5], Step [700/938], Student Loss : 0.2186, Total Loss: 1.1539\n",
      "Epoch [1/5], Step [800/938], Student Loss : 0.1631, Total Loss: 1.0458\n",
      "Epoch [1/5], Step [900/938], Student Loss : 0.4773, Total Loss: 1.4397\n",
      "Epoch [2/5], Step [100/938], Student Loss : 0.3148, Total Loss: 1.6611\n",
      "Epoch [2/5], Step [200/938], Student Loss : 0.3473, Total Loss: 3.0052\n",
      "Epoch [2/5], Step [300/938], Student Loss : 0.5968, Total Loss: 1.6403\n",
      "Epoch [2/5], Step [400/938], Student Loss : 0.0611, Total Loss: 0.9886\n",
      "Epoch [2/5], Step [500/938], Student Loss : 0.2777, Total Loss: 0.6863\n",
      "Epoch [2/5], Step [600/938], Student Loss : 0.1147, Total Loss: 1.2115\n",
      "Epoch [2/5], Step [700/938], Student Loss : 0.2950, Total Loss: 1.3145\n",
      "Epoch [2/5], Step [800/938], Student Loss : 0.1428, Total Loss: 0.8588\n",
      "Epoch [2/5], Step [900/938], Student Loss : 0.2483, Total Loss: 1.2941\n",
      "Epoch [3/5], Step [100/938], Student Loss : 0.0389, Total Loss: 0.7593\n",
      "Epoch [3/5], Step [200/938], Student Loss : 0.0489, Total Loss: 0.6878\n",
      "Epoch [3/5], Step [300/938], Student Loss : 0.0317, Total Loss: 0.6488\n",
      "Epoch [3/5], Step [400/938], Student Loss : 0.0909, Total Loss: 0.9245\n",
      "Epoch [3/5], Step [500/938], Student Loss : 0.0056, Total Loss: 0.3832\n",
      "Epoch [3/5], Step [600/938], Student Loss : 0.0018, Total Loss: 1.0154\n",
      "Epoch [3/5], Step [700/938], Student Loss : 0.1075, Total Loss: 0.7496\n",
      "Epoch [3/5], Step [800/938], Student Loss : 0.2300, Total Loss: 1.1219\n",
      "Epoch [3/5], Step [900/938], Student Loss : 0.2168, Total Loss: 0.7760\n",
      "Epoch [4/5], Step [100/938], Student Loss : 0.0038, Total Loss: 0.4558\n",
      "Epoch [4/5], Step [200/938], Student Loss : 0.3084, Total Loss: 0.5788\n",
      "Epoch [4/5], Step [300/938], Student Loss : 0.1226, Total Loss: 0.4086\n",
      "Epoch [4/5], Step [400/938], Student Loss : 0.1236, Total Loss: 0.5367\n",
      "Epoch [4/5], Step [500/938], Student Loss : 0.1287, Total Loss: 0.3545\n",
      "Epoch [4/5], Step [600/938], Student Loss : 0.0657, Total Loss: 0.2820\n",
      "Epoch [4/5], Step [700/938], Student Loss : 0.0631, Total Loss: 0.4085\n",
      "Epoch [4/5], Step [800/938], Student Loss : 0.1821, Total Loss: 0.6157\n",
      "Epoch [4/5], Step [900/938], Student Loss : 0.0203, Total Loss: 0.4708\n",
      "Epoch [5/5], Step [100/938], Student Loss : 0.0946, Total Loss: 0.5327\n",
      "Epoch [5/5], Step [200/938], Student Loss : 0.0441, Total Loss: 0.3774\n",
      "Epoch [5/5], Step [300/938], Student Loss : 0.0548, Total Loss: 0.3114\n",
      "Epoch [5/5], Step [400/938], Student Loss : 0.0324, Total Loss: 0.4279\n",
      "Epoch [5/5], Step [500/938], Student Loss : 0.0542, Total Loss: 0.3380\n",
      "Epoch [5/5], Step [600/938], Student Loss : 0.3306, Total Loss: 0.4517\n",
      "Epoch [5/5], Step [700/938], Student Loss : 0.2335, Total Loss: 0.9449\n",
      "Epoch [5/5], Step [800/938], Student Loss : 0.0422, Total Loss: 0.4800\n",
      "Epoch [5/5], Step [900/938], Student Loss : 0.0081, Total Loss: 0.6415\n",
      "Student loss: 0.3253565156754727\n",
      "Distillation loss: 1.3241054289870793\n",
      "Total loss: 1.3241054289870793\n",
      "saved model\n",
      "Test Accuracy: 0.9844\n",
      "Test Accuracy: 0.9766\n",
      "Test Accuracy: 0.9740\n",
      "Test Accuracy: 0.9688\n",
      "Test Accuracy: 0.9750\n",
      "Test Accuracy: 0.9688\n",
      "Test Accuracy: 0.9688\n",
      "Test Accuracy: 0.9688\n",
      "Test Accuracy: 0.9705\n",
      "Test Accuracy: 0.9656\n",
      "Test Accuracy: 0.9645\n",
      "Test Accuracy: 0.9609\n",
      "Test Accuracy: 0.9615\n",
      "Test Accuracy: 0.9609\n",
      "Test Accuracy: 0.9583\n",
      "Test Accuracy: 0.9580\n",
      "Test Accuracy: 0.9568\n",
      "Test Accuracy: 0.9566\n",
      "Test Accuracy: 0.9539\n",
      "Test Accuracy: 0.9508\n",
      "Test Accuracy: 0.9494\n",
      "Test Accuracy: 0.9503\n",
      "Test Accuracy: 0.9484\n",
      "Test Accuracy: 0.9479\n",
      "Test Accuracy: 0.9469\n",
      "Test Accuracy: 0.9465\n",
      "Test Accuracy: 0.9456\n",
      "Test Accuracy: 0.9459\n",
      "Test Accuracy: 0.9456\n",
      "Test Accuracy: 0.9458\n",
      "Test Accuracy: 0.9466\n",
      "Test Accuracy: 0.9458\n",
      "Test Accuracy: 0.9465\n",
      "Test Accuracy: 0.9458\n",
      "Test Accuracy: 0.9460\n",
      "Test Accuracy: 0.9457\n",
      "Test Accuracy: 0.9468\n",
      "Test Accuracy: 0.9461\n",
      "Test Accuracy: 0.9463\n",
      "Test Accuracy: 0.9465\n",
      "Test Accuracy: 0.9463\n",
      "Test Accuracy: 0.9464\n",
      "Test Accuracy: 0.9473\n",
      "Test Accuracy: 0.9482\n",
      "Test Accuracy: 0.9490\n",
      "Test Accuracy: 0.9487\n",
      "Test Accuracy: 0.9478\n",
      "Test Accuracy: 0.9479\n",
      "Test Accuracy: 0.9480\n",
      "Test Accuracy: 0.9487\n",
      "Test Accuracy: 0.9488\n",
      "Test Accuracy: 0.9498\n",
      "Test Accuracy: 0.9493\n",
      "Test Accuracy: 0.9497\n",
      "Test Accuracy: 0.9497\n",
      "Test Accuracy: 0.9495\n",
      "Test Accuracy: 0.9498\n",
      "Test Accuracy: 0.9504\n",
      "Test Accuracy: 0.9499\n",
      "Test Accuracy: 0.9495\n",
      "Test Accuracy: 0.9490\n",
      "Test Accuracy: 0.9486\n",
      "Test Accuracy: 0.9484\n",
      "Test Accuracy: 0.9480\n",
      "Test Accuracy: 0.9483\n",
      "Test Accuracy: 0.9479\n",
      "Test Accuracy: 0.9478\n",
      "Test Accuracy: 0.9472\n",
      "Test Accuracy: 0.9475\n",
      "Test Accuracy: 0.9475\n",
      "Test Accuracy: 0.9474\n",
      "Test Accuracy: 0.9473\n",
      "Test Accuracy: 0.9476\n",
      "Test Accuracy: 0.9478\n",
      "Test Accuracy: 0.9483\n",
      "Test Accuracy: 0.9482\n",
      "Test Accuracy: 0.9481\n",
      "Test Accuracy: 0.9477\n",
      "Test Accuracy: 0.9482\n",
      "Test Accuracy: 0.9484\n",
      "Test Accuracy: 0.9491\n",
      "Test Accuracy: 0.9497\n",
      "Test Accuracy: 0.9503\n",
      "Test Accuracy: 0.9509\n",
      "Test Accuracy: 0.9513\n",
      "Test Accuracy: 0.9515\n",
      "Test Accuracy: 0.9520\n",
      "Test Accuracy: 0.9522\n",
      "Test Accuracy: 0.9524\n",
      "Test Accuracy: 0.9526\n",
      "Test Accuracy: 0.9531\n",
      "Test Accuracy: 0.9523\n",
      "Test Accuracy: 0.9514\n",
      "Test Accuracy: 0.9511\n",
      "Test Accuracy: 0.9505\n",
      "Test Accuracy: 0.9505\n",
      "Test Accuracy: 0.9504\n",
      "Test Accuracy: 0.9509\n",
      "Test Accuracy: 0.9514\n",
      "Test Accuracy: 0.9519\n",
      "Test Accuracy: 0.9524\n",
      "Test Accuracy: 0.9527\n",
      "Test Accuracy: 0.9527\n",
      "Test Accuracy: 0.9522\n",
      "Test Accuracy: 0.9527\n",
      "Test Accuracy: 0.9528\n",
      "Test Accuracy: 0.9531\n",
      "Test Accuracy: 0.9536\n",
      "Test Accuracy: 0.9540\n",
      "Test Accuracy: 0.9544\n",
      "Test Accuracy: 0.9547\n",
      "Test Accuracy: 0.9551\n",
      "Test Accuracy: 0.9553\n",
      "Test Accuracy: 0.9555\n",
      "Test Accuracy: 0.9558\n",
      "Test Accuracy: 0.9562\n",
      "Test Accuracy: 0.9562\n",
      "Test Accuracy: 0.9566\n",
      "Test Accuracy: 0.9569\n",
      "Test Accuracy: 0.9573\n",
      "Test Accuracy: 0.9575\n",
      "Test Accuracy: 0.9579\n",
      "Test Accuracy: 0.9580\n",
      "Test Accuracy: 0.9583\n",
      "Test Accuracy: 0.9586\n",
      "Test Accuracy: 0.9588\n",
      "Test Accuracy: 0.9588\n",
      "Test Accuracy: 0.9591\n",
      "Test Accuracy: 0.9593\n",
      "Test Accuracy: 0.9595\n",
      "Test Accuracy: 0.9593\n",
      "Test Accuracy: 0.9594\n",
      "Test Accuracy: 0.9595\n",
      "Test Accuracy: 0.9594\n",
      "Test Accuracy: 0.9597\n",
      "Test Accuracy: 0.9600\n",
      "Test Accuracy: 0.9603\n",
      "Test Accuracy: 0.9606\n",
      "Test Accuracy: 0.9608\n",
      "Test Accuracy: 0.9610\n",
      "Test Accuracy: 0.9610\n",
      "Test Accuracy: 0.9612\n",
      "Test Accuracy: 0.9614\n",
      "Test Accuracy: 0.9616\n",
      "Test Accuracy: 0.9619\n",
      "Test Accuracy: 0.9619\n",
      "Test Accuracy: 0.9622\n",
      "Test Accuracy: 0.9623\n",
      "Test Accuracy: 0.9625\n",
      "Test Accuracy: 0.9625\n",
      "Test Accuracy: 0.9621\n",
      "Test Accuracy: 0.9619\n",
      "Test Accuracy: 0.9615\n",
      "Test Accuracy: 0.9613\n",
      "Test Accuracy: 0.9610\n",
      "Test Accuracy: 0.9609\n",
      "Test Accuracy: 0.9610\n"
     ]
    }
   ],
   "source": [
    "#Train student model on this \n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda')\n",
    "\n",
    "mlp_teacher = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models_shiftinvariantmlp/mlp\")\n",
    "print(\"Invariance of teacher:\" + str(test_IM(test_loader, mlp_teacher)))\n",
    "\n",
    "if TRAIN:\n",
    "    shiftinvmlp_mlp_distiller = Distiller(student=mlp_student, teacher=mlp_teacher, device='cuda', lr=0.001)\n",
    "    shiftinvmlp_mlp_distiller.distill(train_loader, 5, \"saved_models_mlpfromshiftinvariantmlp/\")\n",
    "    shiftinvmlp_mlp_distiller.test_step(test_loader=test_loader)\n",
    "\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    shiftinvmlp_mlp_distiller = Distiller(student=mlp_student, teacher=mlp_teacher, device='cuda', lr=0.001,\n",
    "                        load_student_from_path = 'saved_models_mlpfromshiftinvariantmlp/distiller')\n",
    "    shiftinvmlp_mlp_distiller.test_step(test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9111, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_IM(test_loader, shiftinvmlp_mlp_distiller.get_student())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1400, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_IM(test_loader, shiftinvmlp_mlp_distiller.get_student())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanilla MLP -> 0.9\n",
    "CNN over MLP -> 0.6\n",
    "\n",
    "Self-distilled MLP -> 0.5\n",
    "MLP over MLP -> 0.5\n",
    "\n",
    "distilled MLP over data augmented MLP on non augmented dataset -> 0.48\n",
    "\n",
    "extended MNIST and split the dataset in 2 -> train 2 independent CNNs (or use low teacher fidelity, KL between teachers should be high) get t and t', distill to s and s', compute agreement between t and s' \n",
    "\n",
    "repeat process for all temperatures [1,4,8,16] and all random seeds \n",
    "\n",
    "Data augmented MLP -> 0.09\n",
    "(distilled MLP over data augmented MLP on augmented dataset -> 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change IM measure with temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = [0.34, 0.77, 100]\n",
    "b = [0.77, 0.34, 100]\n",
    "\n",
    "return torch.sum(torch.norm(labels_normal - labels_shifted, dim=1))\n",
    "\n",
    "(a - b )^2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
