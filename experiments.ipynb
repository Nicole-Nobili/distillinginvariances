{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4 exp 4 temps\n",
    "\n",
    "self dist\n",
    "self dist shifted\n",
    "mlp vanilla\n",
    "cnn vanilla\n",
    "cnn mlp \n",
    "cnn stupider + mlp\n",
    "fidelity of mlp to t' cross fidelity wrt first cnn - 1 plot\n",
    "\n",
    "accuracy NLL ECE \n",
    "\n",
    "top1 agreement between teacher and student,\n",
    "KL divergence, invariance metric (crossentropy?) -> show patrick it's better\n",
    "\n",
    "FLOPS!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepspeed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleCNN\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resnet18_mnist\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLP\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistillation_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Distiller\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minvariances_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shift_preserving_shape, test_IM, validate\n",
      "File \u001b[1;32mc:\\Users\\dionys\\Documents\\Master\\HS23\\DL\\distillinginvariances\\distillinginvariances\\models\\mlp.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdeepspeed\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMLP\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     11\u001b[0m         input_dim: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m         from_saved_state_dict: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m     ):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'deepspeed'"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from models.cnn import SimpleCNN\n",
    "from models.resnet import resnet18_mnist\n",
    "from models.mlp import MLP\n",
    "from distillation_utils import Distiller\n",
    "from invariances_utils import shift_preserving_shape, test_IM, validate\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_channels = 1\n",
    "# num_conv_layers = 2\n",
    "num_epochs = 10\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "TRAIN = False\n",
    "device = 'cuda'\n",
    "temperature = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "# Define a custom dataset that combines MNIST and additional data\n",
    "class ShiftAugmentedMNIST(Dataset):\n",
    "    def __init__(self, mnist_dataset, translation_times : int = 5, max_shift : int = 5):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        directions = [\"u\",\"d\",\"l\",\"r\"]\n",
    "        self.translations = []\n",
    "        for i in range(len(self.mnist_dataset)):\n",
    "            img, label = self.mnist_dataset[i]\n",
    "            img = img.squeeze()\n",
    "            for t in range(translation_times):\n",
    "                sh = shift_preserving_shape(img, direction=directions[np.random.randint(0,4)],\n",
    "                                            max_shift=max_shift).unsqueeze(0)\n",
    "                if sh is not None:\n",
    "                    self.translations.append((sh, label))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.mnist_dataset):\n",
    "            return self.mnist_dataset[index]\n",
    "        else:\n",
    "            return self.translations[index - len(self.mnist_dataset)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset) + len(self.translations)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_augmented_dataset = ShiftAugmentedMNIST(train_dataset)\n",
    "train_augmented_loader = DataLoader(dataset=train_augmented_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undistilled MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using softmax\n",
      "Correct normal: 0.9714\n",
      "Correct shifted: 0.2985\n",
      "\n",
      "accu: 0.97153664\n",
      "\n",
      "nlll: 0.10431392\n",
      "\n",
      "ecel: 0.03115510\n",
      "\n",
      "test_IM: 0.89523607\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accu': 0.9715366363525391,\n",
       " 'nlll': 0.1043139174580574,\n",
       " 'ecel': 0.03115510381758213,\n",
       " 'test_IM': tensor(0.8952, device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Loading undistilled MLP\n",
    "if TRAIN:\n",
    "    undistilled_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "        hidden_layers= 4, device='cuda')\n",
    "    criterion_mlp = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_mlp = torch.optim.Adam(undistilled_mlp.parameters(), lr=lr)\n",
    "    undistilled_mlp.training_loop(train_loader=train_loader, optimizer=optimizer_mlp, criterion=criterion_mlp, \n",
    "              num_epochs=5, save_path_folder=\"saved_models_undistilled\")\n",
    "if not TRAIN:\n",
    "    undistilled_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models_undistilled/mlp\")\n",
    "\n",
    "validate(model=undistilled_mlp, weights_file=\"saved_models_undistilled/mlp\", valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undistilled MLP Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading undistilled MLP augmented\n",
    "if TRAIN:\n",
    "    undistilled_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "        hidden_layers= 4, device='cuda')\n",
    "    criterion_mlp = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_mlp = torch.optim.Adam(undistilled_mlp.parameters(), lr=lr)\n",
    "    undistilled_mlp.training_loop(train_loader=train_augmented_loader, optimizer=optimizer_mlp, criterion=criterion_mlp, \n",
    "              num_epochs=5, save_path_folder=\"saved_models_undistilled_augmented\")\n",
    "if not TRAIN:\n",
    "    undistilled_mlp = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device='cuda', from_saved_state_dict=\"saved_models_undistilled_augmented/mlp\")\n",
    "\n",
    "validate(model=undistilled_mlp, weights_file=\"saved_models_undistilled_augmented/mlp\", valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining CNN\n",
    "cnn_path = \"saved_cnn/model_1\"\n",
    "#cnn = SimpleCNN(in_channels=in_channels, num_classes=num_classes, num_conv_layers=num_conv_layers, temperature=temperature).to('cuda:0')\n",
    "cnn = resnet18_mnist().to(device)\n",
    "if TRAIN:\n",
    "    criterion_cnn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "    # model training\n",
    "    prof = deepspeed.profiling.flops.profiler.FlopsProfiler(cnn)\n",
    "    profile_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch == profile_epoch:\n",
    "            prof.start_profile()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            outputs = cnn(images.to(device))\n",
    "            loss = criterion_cnn(outputs, labels.to(device))\n",
    "\n",
    "            optimizer_cnn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_cnn.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "            if epoch == profile_epoch and i == 0:\n",
    "                prof.stop_profile()\n",
    "                flops = prof.get_total_flops()\n",
    "                prof.end_profile()\n",
    "    # Save the trained model\n",
    "    torch.save(cnn.state_dict(), cnn_path)\n",
    "    print(f\"Model saved as {cnn_path}!\")\n",
    "    print(\"Model flops: \", flops)\n",
    "if not TRAIN:\n",
    "    state_dict = torch.load(cnn_path)\n",
    "    cnn.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "validate(model=undistilled_mlp, weights_file=cnn_path, valid_data=test_loader, device=device, is_mlp= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create second \"stupid\" cnn model (at the moment it is the same)\n",
    "#Obtaining CNN\n",
    "cnn_path = \"saved_cnn/model_2\"\n",
    "#cnn = SimpleCNN(in_channels=in_channels, num_classes=num_classes, num_conv_layers=num_conv_layers, temperature=temperature).to('cuda:0')\n",
    "cnn = resnet18_mnist().to(device)\n",
    "if TRAIN:\n",
    "    criterion_cnn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_cnn = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "    # model training\n",
    "    prof = deepspeed.profiling.flops.profiler.FlopsProfiler(cnn)\n",
    "    profile_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch == profile_epoch:\n",
    "            prof.start_profile()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            outputs = cnn(images.to(device))\n",
    "            loss = criterion_cnn(outputs, labels.to(device))\n",
    "\n",
    "            optimizer_cnn.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_cnn.step()\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "            if epoch == profile_epoch and i == 0:\n",
    "                prof.stop_profile()\n",
    "                flops = prof.get_total_flops()\n",
    "                prof.end_profile()\n",
    "    # Save the trained model\n",
    "    torch.save(cnn.state_dict(), cnn_path)\n",
    "    print(f\"Model saved as {cnn_path}!\")\n",
    "    print(\"Model flops: \", flops)\n",
    "if not TRAIN:\n",
    "    state_dict = torch.load(cnn_path)\n",
    "    cnn.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "validate(model=undistilled_mlp, weights_file=cnn_path, valid_data=test_loader, device=device, is_mlp= False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self distilling MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Self distilling MLP (only from unshifted data)\n",
    "save_path_folder = \"saved_models_selfdistill_unshifted\"\n",
    "teacher_path = \"saved_models_undistilled/mlp\"\n",
    "#Self distillation: mlp_student and mlp teacher coincide #TODO CHECK\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_teacher = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device, from_saved_state_dict=teacher_path)\n",
    "\n",
    "if TRAIN:\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001)\n",
    "    selfdistiller.distill(train_loader, 5, save_path_folder) # TODO the model will not be saved in distill() or is it inherited??\n",
    "    # selfdistiller.test_step(test_loader=test_loader) TODO: there is not test_step method?\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher, weights_file=teacher_path, valid_data=test_loader, device=device, is_mlp= True)\n",
    "    validate(model=mlp_student, weights_file=save_path_folder + 'distiller', valid_data=test_loader, device=device, is_mlp= True)\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder + 'distiller')\n",
    "    # selfdistiller.test_step(test_loader=test_loader) TODO: there is not test_step method?\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher, weights_file=teacher_path, valid_data=test_loader, device=device, is_mlp= True)\n",
    "    validate(model=mlp_student, weights_file=save_path_folder + 'distiller', valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self distilling MLP Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Self distilling MLP (augmented)\n",
    "# TODO: Do we use train_loader or train_augmented_loader in distill?\n",
    "save_path_folder = \"saved_models_selfdistill_augmented\"\n",
    "teacher_path = \"saved_models_undistilled_augmented/mlp\"\n",
    "#Self distillation: mlp_student and mlp teacher coincide #TODO CHECK\n",
    "mlp_student = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_teacher = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device, from_saved_state_dict=teacher_path)\n",
    "\n",
    "if TRAIN:\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001)\n",
    "    selfdistiller.distill(train_loader, 5, save_path_folder) # TODO the model is not saved in distill() or is it inherited??\n",
    "    # selfdistiller.test_step(test_loader=test_loader) TODO: there is not test_step method?\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher, weights_file=teacher_path, valid_data=test_loader, device=device, is_mlp= True)\n",
    "    validate(model=mlp_student, weights_file=save_path_folder + 'distiller', valid_data=test_loader, device=device, is_mlp= True)\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    selfdistiller = Distiller(student=mlp_student, teacher=mlp_teacher, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder + 'distiller')\n",
    "    # selfdistiller.test_step(test_loader=test_loader) TODO: there is not test_step method?\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher, weights_file=teacher_path, valid_data=test_loader, device=device, is_mlp= True)\n",
    "    validate(model=mlp_student, weights_file=save_path_folder + 'distiller', valid_data=test_loader, device=device, is_mlp= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distilling CNN_1 -> MLP_1 \n",
    "\n",
    "Distilling CNN_2 -> MLP_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distilling CNN to MLP\n",
    "save_path_folder_1 = \"saved_models_distill_cnn1_to_mlp1\"\n",
    "save_path_folder_2 = \"saved_models_distill_cnn2_to_mlp2\"\n",
    "teacher_path_1 = \"saved_cnn/model_1\"\n",
    "teacher_path_2 = \"saved_cnn/model_2\"\n",
    "\n",
    "mlp_student_1 = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_student_2 = MLP(input_dim = 784, output_dim= num_classes, hidden_size= 2048,\n",
    "            hidden_layers= 4, device=device)\n",
    "\n",
    "mlp_teacher_1 = resnet18_mnist(from_saved_state_dict=teacher_path_1).to(device)\n",
    "\n",
    "#TODO: use different architecture than mlp_teacher_1 (at the moment it is the same)\n",
    "mlp_teacher_2 = resnet18_mnist(from_saved_state_dict=teacher_path_2).to(device)\n",
    "\n",
    "if TRAIN:\n",
    "    selfdistiller = Distiller(student=mlp_student_1, teacher=mlp_teacher_1, device=device, lr=0.001)\n",
    "    selfdistiller.distill(train_loader, 5, save_path_folder) # TODO the model is not saved in distill() or is it inherited??\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher_1, weights_file=teacher_path_1, valid_data=test_loader, device=device, is_mlp= False)\n",
    "    validate(model=mlp_student_1, weights_file=save_path_folder_1 + 'distiller', valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "    selfdistiller = Distiller(student=mlp_student_2, teacher=mlp_teacher_2, device=device, lr=0.001)\n",
    "    selfdistiller.distill(train_loader, 5, save_path_folder) # TODO the model is not saved in distill() or is it inherited??\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher_2, weights_file=teacher_path_2, valid_data=test_loader, device=device, is_mlp= False)\n",
    "    validate(model=mlp_student_2, weights_file=save_path_folder_2 + 'distiller', valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "    selfdistiller = Distiller(student=mlp_student_2, teacher=mlp_teacher_1, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_2 + 'distiller')\n",
    "    selfdistiller.compute_fidelity(test_loader) # compute fidelity of mlp2 and teacher1\n",
    "if not TRAIN:\n",
    "    print(\"Loading params\")\n",
    "    selfdistiller = Distiller(student=mlp_student_1, teacher=mlp_teacher_1, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_1 + 'distiller')\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher_1, weights_file=teacher_path_1, valid_data=test_loader, device=device, is_mlp= False)\n",
    "    validate(model=mlp_student_1, weights_file=save_path_folder_1 + 'distiller', valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "    selfdistiller = Distiller(student=mlp_student_2, teacher=mlp_teacher_2, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_2 + 'distiller')\n",
    "    selfdistiller.compute_fidelity(test_loader)\n",
    "    validate(model=mlp_teacher_2, weights_file=teacher_path_2, valid_data=test_loader, device=device, is_mlp= False)\n",
    "    validate(model=mlp_student_2, weights_file=save_path_folder_2 + 'distiller', valid_data=test_loader, device=device, is_mlp= True)\n",
    "\n",
    "    selfdistiller = Distiller(student=mlp_student_2, teacher=mlp_teacher_1, device=device, lr=0.001,\n",
    "                        load_student_from_path = save_path_folder_2 + 'distiller')\n",
    "    selfdistiller.compute_fidelity(test_loader) # compute fidelity of mlp2 and teacher1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
